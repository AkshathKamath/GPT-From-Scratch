{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "709e8d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f8f9ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implemented using the Attention notebooks in the Attention directory\n",
    "\n",
    "def selfAttention(input_embeddings, W_q, W_k, W_v, W_o):\n",
    "    n = input_embeddings.shape[0]\n",
    "    d_model = input_embeddings.shape[1]\n",
    "    d_k = W_q.shape[1]\n",
    "\n",
    "    Q = torch.matmul(input_embeddings, W_q)\n",
    "    K = torch.matmul(input_embeddings, W_k)\n",
    "    V = torch.matmul(input_embeddings, W_v)\n",
    "\n",
    "    mask  = torch.tril(torch.ones(n, n))\n",
    "\n",
    "    attention_scores = torch.matmul(Q, K.T)\n",
    "    masked_attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "    masked_attention_scores /= torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "    attention_weights = F.softmax(masked_attention_scores, dim=-1)\n",
    "    dropout = nn.Dropout(p=0.2)\n",
    "    attention_weights = dropout(attention_weights)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    final_output = torch.matmul(output, W_o)\n",
    "    final_output = dropout(final_output)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d3b34e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "Number of tokens: 9\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "## Simple tokenization by splitting on spaces, ideally more complex tokenization would be used like BPE or WordPiece\n",
    "sentence = sentence.split()\n",
    "n = len(sentence)\n",
    "\n",
    "print(f\"Tokenized sentence: {sentence}\")\n",
    "print(f\"Number of tokens: {len(sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "402198da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings (4-dimensional):\n",
      "  The : tensor([1.0000, 0.5000, 0.2000, 0.8000])\n",
      "  quick: tensor([0.3000, 1.0000, 0.7000, 0.1000])\n",
      "  brown: tensor([0.6000, 0.2000, 1.0000, 0.4000])\n",
      "  fox : tensor([0.9000, 0.8000, 0.3000, 1.0000])\n",
      "  jumps: tensor([0.4000, 0.6000, 0.8000, 0.2000])\n",
      "  over: tensor([0.7000, 0.3000, 0.5000, 0.9000])\n",
      "  the : tensor([1.0000, 0.5000, 0.2000, 0.8000])\n",
      "  lazy: tensor([0.2000, 0.9000, 0.4000, 0.6000])\n",
      "  dog : tensor([0.8000, 0.4000, 0.9000, 0.3000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Sample word embeddings, ideally these would be learned in the language modelling process or loaded from a pre-trained model like GloVe or Word2Vec\n",
    "\n",
    "# Shape of embeddings: (n, d) where n is number of tokens and d is embedding dimension\n",
    "embeddings = torch.tensor([\n",
    "        [1.0, 0.5, 0.2, 0.8], \n",
    "        [0.3, 1.0, 0.7, 0.1],  \n",
    "        [0.6, 0.2, 1.0, 0.4],  \n",
    "        [0.9, 0.8, 0.3, 1.0],  \n",
    "        [0.4, 0.6, 0.8, 0.2],  \n",
    "        [0.7, 0.3, 0.5, 0.9],  \n",
    "        [1.0, 0.5, 0.2, 0.8],  \n",
    "        [0.2, 0.9, 0.4, 0.6],  \n",
    "        [0.8, 0.4, 0.9, 0.3]  \n",
    "    ])\n",
    "\n",
    "print(\"Word embeddings (4-dimensional):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  {word:4}: {embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b254303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embeddings (Same dimesnions as word embeddings):\n",
      "  Pos 0 (The): tensor([0., 1., 0., 1.])\n",
      "  Pos 1 (quick): tensor([0.1000, 0.9000, 0.1000, 0.9000])\n",
      "  Pos 2 (brown): tensor([0.2000, 0.8000, 0.2000, 0.8000])\n",
      "  Pos 3 (fox): tensor([0.3000, 0.7000, 0.3000, 0.7000])\n",
      "  Pos 4 (jumps): tensor([0.4000, 0.6000, 0.4000, 0.6000])\n",
      "  Pos 5 (over): tensor([0.5000, 0.5000, 0.5000, 0.5000])\n",
      "  Pos 6 (the): tensor([0.6000, 0.4000, 0.6000, 0.4000])\n",
      "  Pos 7 (lazy): tensor([0.7000, 0.3000, 0.7000, 0.3000])\n",
      "  Pos 8 (dog): tensor([0.8000, 0.2000, 0.8000, 0.2000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Sample positional encodings, typically these would be generated using math functions or learned during training or RoPE\n",
    "\n",
    "positional_embeddings = torch.tensor([\n",
    "    [0.0, 1.0, 0.0, 1.0],  \n",
    "    [0.1, 0.9, 0.1, 0.9],  \n",
    "    [0.2, 0.8, 0.2, 0.8],  \n",
    "    [0.3, 0.7, 0.3, 0.7],  \n",
    "    [0.4, 0.6, 0.4, 0.6],  \n",
    "    [0.5, 0.5, 0.5, 0.5],  \n",
    "    [0.6, 0.4, 0.6, 0.4],  \n",
    "    [0.7, 0.3, 0.7, 0.3],  \n",
    "    [0.8, 0.2, 0.8, 0.2]   \n",
    "])\n",
    "\n",
    "print(\"Positional embeddings (Same dimesnions as word embeddings):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  Pos {i} ({word}): {positional_embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0066e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings (word + positional):\n",
      "  The : tensor([0.0000, 1.8750, 0.2500, 2.2500])\n",
      "  quick: tensor([0.5000, 2.3750, 1.0000, 1.2500])\n",
      "  brown: tensor([1.0000, 1.2500, 1.5000, 1.5000])\n",
      "  fox : tensor([1.5000, 1.8750, 0.7500, 2.1250])\n",
      "  jumps: tensor([1.0000, 1.5000, 1.5000, 0.0000])\n",
      "  over: tensor([1.5000, 0.0000, 1.2500, 1.7500])\n",
      "  the : tensor([2.0000, 1.1250, 1.0000, 1.5000])\n",
      "  lazy: tensor([1.1250, 1.5000, 1.3750, 1.1250])\n",
      "  dog : tensor([2.0000, 0.7500, 2.1250, 0.6250])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## The final input to the Attention block is the sum of the word embeddings and positional encodings\n",
    "\n",
    "input_embeddings = embeddings + positional_embeddings\n",
    "dropout = nn.Dropout(p=0.2)\n",
    "input_embeddings = dropout(input_embeddings)\n",
    "\n",
    "print(\"Input embeddings (word + positional):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  {word:4}: {input_embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a30e7b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = input_embeddings.shape[1]  # Embedding dimension\n",
    "d_k = 3 # Dimension of keys and queries (generally kept smaller to make Q, K and V matrices low rank for efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6bf43ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_q (Query weights) shape: torch.Size([4, 3])\n",
      "tensor([[ 0.1010,  0.0386,  0.0703],\n",
      "        [ 0.0691, -0.3369, -0.0559],\n",
      "        [ 0.6625, -0.1914,  0.1385],\n",
      "        [ 0.0802,  0.1605,  0.2428]])\n",
      "\n",
      "W_k (Key weights) shape: torch.Size([4, 3])\n",
      "tensor([[ 0.3331, -0.5069, -0.2967],\n",
      "        [ 0.2874,  0.3966,  0.2452],\n",
      "        [-0.2298, -0.2252,  0.4058],\n",
      "        [ 0.2059, -0.0983,  0.2385]])\n",
      "\n",
      "W_v (Value weights) shape: torch.Size([4, 3])\n",
      "tensor([[ 0.0845,  0.0168,  0.1568],\n",
      "        [-0.0715, -0.0150,  0.1579],\n",
      "        [-0.0025,  0.2187,  0.0399],\n",
      "        [ 0.2592, -0.3047, -0.2666]])\n",
      "\n",
      "W_o (Output projection weights) shape: torch.Size([3, 4])\n",
      "tensor([[ 0.0449, -0.0627, -0.1161,  0.2974],\n",
      "        [ 0.1404, -0.0615, -0.2223,  0.1086],\n",
      "        [ 0.5760, -0.0676, -0.1025,  0.0912]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)  # For reproducible results\n",
    "\n",
    "## Shape of the Q, K and V matrices is d x d_k and for the output projection matrix is d_k x d to project the attention output back to d dimensions\n",
    "W_q = torch.randn(d_model, d_k, dtype=torch.float32) * 0.3  \n",
    "W_k = torch.randn(d_model, d_k, dtype=torch.float32) * 0.3  \n",
    "W_v = torch.randn(d_model, d_k, dtype=torch.float32) * 0.3  \n",
    "W_o = torch.randn(d_k, d_model, dtype=torch.float32) * 0.3 \n",
    "\n",
    "print(f\"W_q (Query weights) shape: {W_q.shape}\")\n",
    "print(W_q)\n",
    "print(f\"\\nW_k (Key weights) shape: {W_k.shape}\")\n",
    "print(W_k)\n",
    "print(f\"\\nW_v (Value weights) shape: {W_v.shape}\")\n",
    "print(W_v)\n",
    "print()\n",
    "\n",
    "print(f\"W_o (Output projection weights) shape: {W_o.shape}\")\n",
    "print(W_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b45bd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention output shape: torch.Size([9, 4])\n",
      "tensor([[-3.7750e-01,  0.0000e+00,  0.0000e+00,  5.4733e-02],\n",
      "        [-1.2751e-01,  1.5926e-02,  0.0000e+00,  6.7920e-02],\n",
      "        [-1.2334e-01,  8.3253e-03,  5.6893e-02,  7.0013e-02],\n",
      "        [-8.0065e-02, -3.1087e-04,  6.1870e-02,  1.1917e-01],\n",
      "        [ 5.2497e-02, -1.8745e-02, -1.0964e-02,  9.9666e-02],\n",
      "        [-5.4889e-02, -5.5895e-03,  3.0832e-02,  1.0706e-01],\n",
      "        [ 4.8455e-02, -2.3002e-02, -1.1746e-02,  1.3226e-01],\n",
      "        [ 1.6850e-02, -1.3240e-02,  1.1290e-02,  1.0419e-01],\n",
      "        [ 0.0000e+00, -2.0309e-02,  1.1206e-03,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "attention_output = selfAttention(input_embeddings, W_q, W_k, W_v, W_o)\n",
    "\n",
    "print(\"\\nAttention output shape:\", attention_output.shape)\n",
    "print(attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92c485ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Residual output shape: torch.Size([9, 4])\n",
      "tensor([[-0.3775,  1.8750,  0.2500,  2.3047],\n",
      "        [ 0.3725,  2.3909,  1.0000,  1.3179],\n",
      "        [ 0.8767,  1.2583,  1.5569,  1.5700],\n",
      "        [ 1.4199,  1.8747,  0.8119,  2.2442],\n",
      "        [ 1.0525,  1.4813,  1.4890,  0.0997],\n",
      "        [ 1.4451, -0.0056,  1.2808,  1.8571],\n",
      "        [ 2.0485,  1.1020,  0.9883,  1.6323],\n",
      "        [ 1.1418,  1.4868,  1.3863,  1.2292],\n",
      "        [ 2.0000,  0.7297,  2.1261,  0.6250]])\n"
     ]
    }
   ],
   "source": [
    "## Adding residual connection where the input embeddings before the attention block are added to the attention output\n",
    "residual_output = attention_output + input_embeddings\n",
    "\n",
    "print(\"\\nResidual output shape:\", residual_output.shape)\n",
    "print(residual_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04dc474d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gamma (scaling factor) shape: torch.Size([4])\n",
      "tensor([1., 1., 1., 1.])\n",
      "\n",
      "Beta (shifting factor) shape: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "## Learnable parameters for layer normalization, gamma is the scaling factor and beta is the shifting factor\n",
    "gamma = torch.ones(d_model) # Shape: (d,) for every embedding dimension\n",
    "beta = torch.zeros(d_model) # Shape: (d,) for every embedding dimension\n",
    "\n",
    "print(\"\\nGamma (scaling factor) shape:\", gamma.shape)\n",
    "print(gamma)\n",
    "print(\"\\nBeta (shifting factor) shape:\", beta.shape)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c01b8516",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now applying layer normalization to the residual output (Post-LN). Recent approaches use Pre-LN for better training stability\n",
    "def layerNorm(residual_output, gamma, beta, eps = 1e-5,):\n",
    "    ## For evert token (row), we calculate the mean and variance across the embedding dimension\n",
    "    means = torch.mean(residual_output, dim=-1, keepdim=True) # Shape (n, 1)\n",
    "    print(f\"Means shape: {means.shape}\")\n",
    "    variances = torch.var(residual_output, dim=-1, keepdim=True, unbiased=False) # Shape (n, 1)\n",
    "    print(f\"Variances shape: {variances.shape}\")\n",
    "\n",
    "    ## Normalizing the residual output i.e. making it zero mean and unit variance\n",
    "    normalized = (residual_output - means) / torch.sqrt(variances + eps) # Shape (n, d)\n",
    "    print(f\"Normalized shape: {normalized.shape}\")\n",
    "\n",
    "    ln_output = normalized * gamma + beta # Shape (n, d) after broadcasting of gamma and beta. For each token (row), we scale and shift the normalized values by element-wise multiplication and addition of the corresponding gamma and beta values for that embedding dimension (column)\n",
    "    return ln_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba815424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means shape: torch.Size([9, 1])\n",
      "Variances shape: torch.Size([9, 1])\n",
      "Normalized shape: torch.Size([9, 4])\n",
      "\n",
      "Final output shape: torch.Size([9, 4])\n",
      "tensor([[-1.2529,  0.7766, -0.6875,  1.1638],\n",
      "        [-1.2283,  1.5330, -0.3698,  0.0651],\n",
      "        [-1.5540, -0.2024,  0.8550,  0.9014],\n",
      "        [-0.3137,  0.5368, -1.4510,  1.2279],\n",
      "        [ 0.0387,  0.7965,  0.8103, -1.6454],\n",
      "        [ 0.4319, -1.6515,  0.1960,  1.0235],\n",
      "        [ 1.4223, -0.8001, -1.0672,  0.4450],\n",
      "        [-1.2618,  1.3107,  0.5614, -0.6103],\n",
      "        [ 0.9058, -0.9212,  1.0872, -1.0718]])\n"
     ]
    }
   ],
   "source": [
    "final_output = layerNorm(residual_output, gamma, beta)\n",
    "\n",
    "print(\"\\nFinal output shape:\", final_output.shape)\n",
    "print(final_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
