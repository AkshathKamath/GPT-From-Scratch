{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "470550f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"The old wizard lived in a tall tower. Every morning he would wake up early and look out his window. \n",
    "    From his window he could see the entire village below. The village was small but busy. \n",
    "    People walked through the streets carrying baskets. The baskets were filled with fresh bread and fruit.\n",
    "    \n",
    "    One day the wizard noticed something strange. A large dragon was flying toward the village. \n",
    "    The dragon was enormous and had bright red scales. The wizard knew he had to act quickly.\n",
    "    He grabbed his magic wand from the wooden table. The wand was old but very powerful.\n",
    "    \n",
    "    The wizard pointed the wand at the dragon and spoke a magic spell. The spell created a bright light.\n",
    "    The light surrounded the dragon and made it disappear. The village was safe once again.\n",
    "    The people in the village cheered and thanked the brave wizard.\n",
    "    \n",
    "    After the adventure the wizard returned to his tower. He was tired but happy. \n",
    "    He had protected the village and its people. The wizard knew that tomorrow might bring new challenges.\n",
    "    But for now he could rest peacefully in his tall tower.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e78975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1b8f85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464, 1468, 18731, 5615, 287, 257, 7331, 10580, 13, 3887]\n",
      "257\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(sample_text)\n",
    "\n",
    "print(enc_text[:10])\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b38f3e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = [] # The input dataset containing sequences of tokens of context size length\n",
    "        self.target_ids = [] # The target dataset containing corresponding next tokens, shifted by 1 (sliding window)\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the text into overlapping sequences of max_length (context window size)\n",
    "        # Stride is used to define, the next input i.e if the text as token ids was 1, 2, 3, 4, 5, 6, 7, 8, 9 and context size was 4 then 1st input would be [1, 2, 3, 4] and target would be [2, 3, 4, 5]. Stride of 1 would mean the next input is [2, 3, 4, 5] and the target would be [3, 4, 5, 6] i.e. low stride means there would be overlap in input tensors and can lead to overfitting. Stride of 4 would mean next input would be [5, 6, 7, 8] and target would be [6, 7, 8, 9]. This would reduce computation by reducing overlap.\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1] # Slided by 1 to create the target\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    ## Necessary functions to create a dataloader\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids) ## Return number of input-target pairs (N x context window), returns the N\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c310102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = TextDataset(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    # \n",
    "    dataloader = DataLoader(\n",
    "        dataset, # Uses the getitem func to iterate over every input\n",
    "        batch_size=batch_size, # Batches multiple inputs together into 1 batch for parallel processing (entire forward pass and then backward pass is done on entire batch), low batch size can cause noise updates and higher batch sizes can be slow and memory expenise\n",
    "        shuffle=shuffle, # Randomize order of samples in each batch\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d10be283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  464,  1468, 18731,  5615]]), tensor([[ 1468, 18731,  5615,   287]])]\n",
      "[tensor([[ 1468, 18731,  5615,   287]]), tensor([[18731,  5615,   287,   257]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    sample_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ") # Batch size of 1 and context window of 4 with stride of 1\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27692eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[  464,  1468, 18731,  5615],\n",
      "        [  287,   257,  7331, 10580],\n",
      "        [   13,  3887,  3329,   339],\n",
      "        [  561,  7765,   510,  1903],\n",
      "        [  290,   804,   503,   465],\n",
      "        [ 4324,    13,   220,   198],\n",
      "        [  220,   220,   220,  3574],\n",
      "        [  465,  4324,   339,   714]])\n",
      "\n",
      "Targets:\n",
      " tensor([[ 1468, 18731,  5615,   287],\n",
      "        [  257,  7331, 10580,    13],\n",
      "        [ 3887,  3329,   339,   561],\n",
      "        [ 7765,   510,  1903,   290],\n",
      "        [  804,   503,   465,  4324],\n",
      "        [   13,   220,   198,   220],\n",
      "        [  220,   220,  3574,   465],\n",
      "        [ 4324,   339,   714,   766]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(sample_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
