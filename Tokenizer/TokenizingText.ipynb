{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dda5ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1098\n",
      "The old wizard lived in a tall tower. Every morning he would wake up early and look out his window.\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"The old wizard lived in a tall tower. Every morning he would wake up early and look out his window. \n",
    "    From his window he could see the entire village below. The village was small but busy. \n",
    "    People walked through the streets carrying baskets. The baskets were filled with fresh bread and fruit.\n",
    "    \n",
    "    One day the wizard noticed something strange. A large dragon was flying toward the village. \n",
    "    The dragon was enormous and had bright red scales. The wizard knew he had to act quickly.\n",
    "    He grabbed his magic wand from the wooden table. The wand was old but very powerful.\n",
    "    \n",
    "    The wizard pointed the wand at the dragon and spoke a magic spell. The spell created a bright light.\n",
    "    The light surrounded the dragon and made it disappear. The village was safe once again.\n",
    "    The people in the village cheered and thanked the brave wizard.\n",
    "    \n",
    "    After the adventure the wizard returned to his tower. He was tired but happy. \n",
    "    He had protected the village and its people. The wizard knew that tomorrow might bring new challenges.\n",
    "    But for now he could rest peacefully in his tall tower.\"\"\"\n",
    "\n",
    "print(len(sample_text)) # Total characters\n",
    "print(sample_text[:99]) # 1st 100 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e201d5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text) #Splitting on space\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07fee73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n",
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text) #Splitting on space and ,.\n",
    "print(result) ## Contains whitespace chars\n",
    "\n",
    "# To remove them\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)\n",
    "\n",
    "# When developing a simple tokenizer, whether we should encode whitespaces as\n",
    "# separate characters or just remove them depends on our application and its\n",
    "# requirements. Removing whitespaces reduces the memory and computing\n",
    "# requirements. However, keeping whitespaces can be useful if we train models that\n",
    "# are sensitive to the exact structure of the text (for example, Python code, which is\n",
    "# sensitive to indentation and spacing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1727245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) ## Handling tokens for more symbols with ,.\n",
    "result = [item.strip() for item in result if item.strip()] ##Remove any whitespace around the token\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45365297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'old', 'wizard', 'lived', 'in', 'a', 'tall', 'tower', '.', 'Every', 'morning', 'he', 'would', 'wake', 'up', 'early', 'and', 'look', 'out', 'his', 'window', '.', 'From', 'his', 'window', 'he', 'could', 'see', 'the', 'entire']\n",
      "209\n"
     ]
    }
   ],
   "source": [
    "## Applying this tokenizer to the sample text\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', sample_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])\n",
    "print(len(preprocessed)) ## The text in form of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e25c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n",
      "['.', 'A', 'After', 'But', 'Every']\n"
     ]
    }
   ],
   "source": [
    "## Converting tokens to IDs\n",
    "all_words = sorted(set(preprocessed)) ## Vocabulary is sorted list (set) of all unique tokens\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size) ## The number of unique tokens\n",
    "print(all_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5602f1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "## We assign every unique token a unique integer called token ID\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)} ## Vocab is a dictionary of all unique tokens and their corresponding token IDs\n",
    "## Encoding the tokens to IDs for input to LLM, need a decoder to convert token IDs back to tokens after LLM outputs\n",
    "print(vocab['The'])\n",
    "print(vocab['he'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ed7cd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('.', 0)\n",
      "('A', 1)\n",
      "('After', 2)\n",
      "('But', 3)\n",
      "('Every', 4)\n",
      "('From', 5)\n",
      "('He', 6)\n",
      "('One', 7)\n",
      "('People', 8)\n",
      "('The', 9)\n",
      "('a', 10)\n",
      "('act', 11)\n",
      "('adventure', 12)\n",
      "('again', 13)\n",
      "('and', 14)\n",
      "('at', 15)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c2daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab ## Token to ID mapping\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()} ## ID to token mapping for decoder\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) ## Split the text into tokens\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip() ## Remove whitespaces from the tokens\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed] ## Using the vocab we generated, assign token ID for each token in the text\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) ## ID to token and joined with a space\n",
    "        \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) ## Replace spaces before the specified punctuations like .,?\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "698b3cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SimpleTokenizer at 0x106a813d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer(vocab)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee84f379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 29, 87, 104, 59, 77, 80, 0, 1, 50, 31, 100, 36, 92, 87, 96, 0]\n",
      "One day the wizard noticed something strange. A large dragon was flying toward the village.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"One day the wizard noticed something strange. A large dragon was flying toward the village.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "\n",
    "print(tokenizer.decode(ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416692ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## For words not in the vocab\u001b[39;00m\n\u001b[32m      3\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mSimpleTokenizer.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.:;?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text) \u001b[38;5;66;03m## Split the text into tokens\u001b[39;00m\n\u001b[32m      9\u001b[39m preprocessed = [\n\u001b[32m     10\u001b[39m     item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip() \u001b[38;5;66;03m## Remove whitespaces from the tokens\u001b[39;00m\n\u001b[32m     11\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed] \u001b[38;5;66;03m## Using the vocab we generated, assign token ID for each token in the text\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "## For words not in the vocab\n",
    "\n",
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))\n",
    "\n",
    "## Hence, we need a large and diverse vocab to train our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6c4f4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      "107\n",
      "109\n"
     ]
    }
   ],
   "source": [
    "## Adding special context tokens to handle such situations\n",
    "# <unk> to represent words that are not in the vocab\n",
    "# <endOfText> added at the end of different text sources to separate them (like one from wikeipedia article, one linkedin post). Helps LLM have better understanding of text\n",
    "\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"]) ## Add these 2 tokens to the list of all unique tokens\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)} ## Create the vocab\n",
    "print(vocab['<|unk|>'])\n",
    "print(vocab['<|endoftext|>'])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2955d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]       \n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed ## If the token is not in the vocab, we replace it with unk token\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8a67a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SimpleTokenizerV2 at 0x106e192b0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76258156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, does the wizard like tea? <|endoftext|> He had protected the village and its people.\n",
      "[108, 108, 108, 87, 104, 108, 108, 108, 107, 6, 42, 68, 87, 96, 14, 48, 65, 0]\n",
      "<|unk|> <|unk|> <|unk|> the wizard <|unk|> <|unk|> <|unk|> <|endoftext|> He had protected the village and its people.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, does the wizard like tea?\"\n",
    "text2 = \"He had protected the village and its people.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d73c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## There is also BOS (beginning of sequence) token, EOS (End of text) and PAD token, however GPT only uses endoftext token rather than these. It uses BPE instead of using unk token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
