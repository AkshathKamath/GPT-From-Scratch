{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4172ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8370efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implemented using the Attention notebooks in the Attention directory\n",
    "\n",
    "def selfAttention(input_embeddings, W_q, W_k, W_v, W_o):\n",
    "    n = input_embeddings.shape[0]\n",
    "    d_model = input_embeddings.shape[1]\n",
    "    d_k = W_q.shape[1]\n",
    "\n",
    "    Q = torch.matmul(input_embeddings, W_q)\n",
    "    K = torch.matmul(input_embeddings, W_k)\n",
    "    V = torch.matmul(input_embeddings, W_v)\n",
    "    dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    mask  = torch.tril(torch.ones(n, n))\n",
    "\n",
    "    attention_scores = torch.matmul(Q, K.T)\n",
    "    masked_attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "    masked_attention_scores /= torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "    attention_weights = F.softmax(masked_attention_scores, dim=-1)\n",
    "    attention_weights = dropout(attention_weights)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    final_output = torch.matmul(output, W_o)\n",
    "    final_output = dropout(final_output)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6648c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implemented using the LayerNorm notebook in the LayerNorm directory\n",
    "def residualPlusLayerNorm(attention_output, input_embeddings, gamma, beta, eps = 1e-5,):\n",
    "    residual_output = attention_output + input_embeddings\n",
    "\n",
    "    means = torch.mean(residual_output, dim=-1, keepdim=True) # Shape (n, 1)\n",
    "    variances = torch.var(residual_output, dim=-1, keepdim=True, unbiased=False) # Shape (n, 1)\n",
    "    normalized = (residual_output - means) / torch.sqrt(variances + eps) # Shape (n, d)\n",
    "\n",
    "    ln_output = normalized * gamma + beta\n",
    "    return ln_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb5b92c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "Number of tokens: 9\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "## Simple tokenization by splitting on spaces, ideally more complex tokenization would be used like BPE or WordPiece\n",
    "sentence = sentence.split()\n",
    "n = len(sentence)\n",
    "\n",
    "print(f\"Tokenized sentence: {sentence}\")\n",
    "print(f\"Number of tokens: {len(sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4dc1937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings (4-dimensional):\n",
      "  The : tensor([1.0000, 0.5000, 0.2000, 0.8000])\n",
      "  quick: tensor([0.3000, 1.0000, 0.7000, 0.1000])\n",
      "  brown: tensor([0.6000, 0.2000, 1.0000, 0.4000])\n",
      "  fox : tensor([0.9000, 0.8000, 0.3000, 1.0000])\n",
      "  jumps: tensor([0.4000, 0.6000, 0.8000, 0.2000])\n",
      "  over: tensor([0.7000, 0.3000, 0.5000, 0.9000])\n",
      "  the : tensor([1.0000, 0.5000, 0.2000, 0.8000])\n",
      "  lazy: tensor([0.2000, 0.9000, 0.4000, 0.6000])\n",
      "  dog : tensor([0.8000, 0.4000, 0.9000, 0.3000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Sample word embeddings, ideally these would be learned in the language modelling process or loaded from a pre-trained model like GloVe or Word2Vec\n",
    "\n",
    "# Shape of embeddings: (n, d) where n is number of tokens and d is embedding dimension\n",
    "embeddings = torch.tensor([\n",
    "        [1.0, 0.5, 0.2, 0.8], \n",
    "        [0.3, 1.0, 0.7, 0.1],  \n",
    "        [0.6, 0.2, 1.0, 0.4],  \n",
    "        [0.9, 0.8, 0.3, 1.0],  \n",
    "        [0.4, 0.6, 0.8, 0.2],  \n",
    "        [0.7, 0.3, 0.5, 0.9],  \n",
    "        [1.0, 0.5, 0.2, 0.8],  \n",
    "        [0.2, 0.9, 0.4, 0.6],  \n",
    "        [0.8, 0.4, 0.9, 0.3]  \n",
    "    ])\n",
    "\n",
    "print(\"Word embeddings (4-dimensional):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  {word:4}: {embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26ec4ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embeddings (Same dimesnions as word embeddings):\n",
      "  Pos 0 (The): tensor([0., 1., 0., 1.])\n",
      "  Pos 1 (quick): tensor([0.1000, 0.9000, 0.1000, 0.9000])\n",
      "  Pos 2 (brown): tensor([0.2000, 0.8000, 0.2000, 0.8000])\n",
      "  Pos 3 (fox): tensor([0.3000, 0.7000, 0.3000, 0.7000])\n",
      "  Pos 4 (jumps): tensor([0.4000, 0.6000, 0.4000, 0.6000])\n",
      "  Pos 5 (over): tensor([0.5000, 0.5000, 0.5000, 0.5000])\n",
      "  Pos 6 (the): tensor([0.6000, 0.4000, 0.6000, 0.4000])\n",
      "  Pos 7 (lazy): tensor([0.7000, 0.3000, 0.7000, 0.3000])\n",
      "  Pos 8 (dog): tensor([0.8000, 0.2000, 0.8000, 0.2000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Sample positional encodings, typically these would be generated using math functions or learned during training or RoPE\n",
    "\n",
    "positional_embeddings = torch.tensor([\n",
    "    [0.0, 1.0, 0.0, 1.0],  \n",
    "    [0.1, 0.9, 0.1, 0.9],  \n",
    "    [0.2, 0.8, 0.2, 0.8],  \n",
    "    [0.3, 0.7, 0.3, 0.7],  \n",
    "    [0.4, 0.6, 0.4, 0.6],  \n",
    "    [0.5, 0.5, 0.5, 0.5],  \n",
    "    [0.6, 0.4, 0.6, 0.4],  \n",
    "    [0.7, 0.3, 0.7, 0.3],  \n",
    "    [0.8, 0.2, 0.8, 0.2]   \n",
    "])\n",
    "\n",
    "print(\"Positional embeddings (Same dimesnions as word embeddings):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  Pos {i} ({word}): {positional_embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e7ec4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings (word + positional):\n",
      "  The : tensor([1.2500, 1.8750, 0.0000, 2.2500])\n",
      "  quick: tensor([0.5000, 2.3750, 0.0000, 1.2500])\n",
      "  brown: tensor([1.0000, 1.2500, 1.5000, 1.5000])\n",
      "  fox : tensor([1.5000, 0.0000, 0.0000, 2.1250])\n",
      "  jumps: tensor([1.0000, 0.0000, 1.5000, 1.0000])\n",
      "  over: tensor([0.0000, 1.0000, 0.0000, 1.7500])\n",
      "  the : tensor([2.0000, 1.1250, 1.0000, 1.5000])\n",
      "  lazy: tensor([0.0000, 1.5000, 1.3750, 1.1250])\n",
      "  dog : tensor([0.0000, 0.0000, 2.1250, 0.0000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## The final input to the Attention block is the sum of the word embeddings and positional encodings\n",
    "dropout = nn.Dropout(p=0.2)\n",
    "input_embeddings = embeddings + positional_embeddings\n",
    "input_embeddings = dropout(input_embeddings)\n",
    "\n",
    "print(\"Input embeddings (word + positional):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  {word:4}: {input_embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a342beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = input_embeddings.shape[1]  # Embedding dimension\n",
    "d_k = 3 # Dimension of keys and queries (generally kept smaller to make Q, K and V matrices low rank for efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f25bee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_q (Query weights) shape: torch.Size([4, 3])\n",
      "tensor([[ 0.1010,  0.0386,  0.0703],\n",
      "        [ 0.0691, -0.3369, -0.0559],\n",
      "        [ 0.6625, -0.1914,  0.1385],\n",
      "        [ 0.0802,  0.1605,  0.2428]])\n",
      "\n",
      "W_k (Key weights) shape: torch.Size([4, 3])\n",
      "tensor([[ 0.3331, -0.5069, -0.2967],\n",
      "        [ 0.2874,  0.3966,  0.2452],\n",
      "        [-0.2298, -0.2252,  0.4058],\n",
      "        [ 0.2059, -0.0983,  0.2385]])\n",
      "\n",
      "W_v (Value weights) shape: torch.Size([4, 3])\n",
      "tensor([[ 0.0845,  0.0168,  0.1568],\n",
      "        [-0.0715, -0.0150,  0.1579],\n",
      "        [-0.0025,  0.2187,  0.0399],\n",
      "        [ 0.2592, -0.3047, -0.2666]])\n",
      "\n",
      "W_o (Output projection weights) shape: torch.Size([3, 4])\n",
      "tensor([[ 0.0449, -0.0627, -0.1161,  0.2974],\n",
      "        [ 0.1404, -0.0615, -0.2223,  0.1086],\n",
      "        [ 0.5760, -0.0676, -0.1025,  0.0912]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)  # For reproducible results\n",
    "\n",
    "## Shape of the Q, K and V matrices is d x d_k and for the output projection matrix is d_k x d to project the attention output back to d dimensions\n",
    "W_q = torch.randn(d_model, d_k, dtype=torch.float32) * 0.3  \n",
    "W_k = torch.randn(d_model, d_k, dtype=torch.float32) * 0.3  \n",
    "W_v = torch.randn(d_model, d_k, dtype=torch.float32) * 0.3  \n",
    "W_o = torch.randn(d_k, d_model, dtype=torch.float32) * 0.3 \n",
    "\n",
    "print(f\"W_q (Query weights) shape: {W_q.shape}\")\n",
    "print(W_q)\n",
    "print(f\"\\nW_k (Key weights) shape: {W_k.shape}\")\n",
    "print(W_k)\n",
    "print(f\"\\nW_v (Value weights) shape: {W_v.shape}\")\n",
    "print(W_v)\n",
    "print()\n",
    "\n",
    "print(f\"W_o (Output projection weights) shape: {W_o.shape}\")\n",
    "print(W_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "462aeb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention output shape: torch.Size([9, 4])\n",
      "tensor([[-0.2100,  0.0000,  0.0000,  0.1249],\n",
      "        [-0.1051,  0.0165,  0.0000,  0.0878],\n",
      "        [-0.0847,  0.0023,  0.0583,  0.0979],\n",
      "        [-0.1094,  0.0071,  0.0883,  0.1120],\n",
      "        [-0.0686, -0.0039,  0.0128,  0.0782],\n",
      "        [-0.1572,  0.0067,  0.0631,  0.1061],\n",
      "        [-0.0728, -0.0029,  0.0369,  0.1038],\n",
      "        [-0.0702, -0.0043,  0.0421,  0.1188],\n",
      "        [-0.0000,  0.0010,  0.0579,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "attention_output = selfAttention(input_embeddings, W_q, W_k, W_v, W_o)\n",
    "\n",
    "print(\"\\nAttention output shape:\", attention_output.shape)\n",
    "print(attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "881a1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The parameters for the layer norm layer that will be applied after the attention layer\n",
    "gamma1 = torch.ones(d_model, dtype=torch.float32)\n",
    "beta1 = torch.zeros(d_model, dtype=torch.float32)\n",
    "\n",
    "## The parameters for the layer norm layer that will be applied after the feed forward layer\n",
    "gamma2 = torch.ones(d_model, dtype=torch.float32)\n",
    "beta2 = torch.zeros(d_model, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f5fe6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LayerNorm output shape: torch.Size([9, 4])\n",
      "tensor([[-0.3138,  0.6138, -1.4690,  1.1691],\n",
      "        [-0.6887,  1.4729, -1.1162,  0.3321],\n",
      "        [-1.5128, -0.2863,  0.8275,  0.9716],\n",
      "        [ 0.4929, -0.9902, -0.9031,  1.4003],\n",
      "        [ 0.0936, -1.5974,  1.1447,  0.3591],\n",
      "        [-1.0595,  0.3924, -0.7847,  1.4518],\n",
      "        [ 1.3911, -0.8280, -1.0627,  0.4997],\n",
      "        [-1.7142,  0.7445,  0.6209,  0.3488],\n",
      "        [-0.5777, -0.5767,  1.7320, -0.5777]])\n"
     ]
    }
   ],
   "source": [
    "layernorm_output = residualPlusLayerNorm(attention_output, input_embeddings, gamma1, beta1)\n",
    "\n",
    "print(\"\\nLayerNorm output shape:\", layernorm_output.shape)\n",
    "print(layernorm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db48102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W_ff1 (Feed forward layer 1 weights) shape: torch.Size([4, 16])\n",
      "tensor([[-0.0667, -0.3741, -0.1459, -0.1008,  0.0110,  0.1480,  0.2656,  0.0547,\n",
      "          0.5679,  0.1334,  0.0409,  0.0926,  0.4985,  0.0525,  0.1825,  0.4893],\n",
      "        [ 0.8556, -0.2231,  0.0586, -0.4005,  0.1184,  0.5118, -0.2382,  0.1126,\n",
      "          0.0229, -0.0638, -0.1699,  0.1197,  0.4108, -0.0756,  0.5701,  0.5085],\n",
      "        [ 0.0917,  0.0873,  0.1226, -0.3783,  0.2750, -0.0084, -0.0655,  0.0499,\n",
      "         -0.0261, -0.3541,  0.4638,  0.1634,  0.2980,  0.1520, -0.0419, -0.3542],\n",
      "        [ 0.5943, -0.0314,  0.1471, -0.1312, -0.3660, -0.1756,  0.1999, -0.0223,\n",
      "          0.2980,  0.0809, -0.5495,  0.1071,  0.2743,  0.6565, -0.2955, -0.7465]])\n",
      "\n",
      "b_ff1 (Feed forward layer 1 bias) shape: torch.Size([16])\n",
      "tensor([-0.1424, -0.0696, -0.0318,  0.1215,  0.1420, -0.0055,  0.0025, -0.1064,\n",
      "        -0.0364, -0.0099,  0.0311,  0.0371,  0.0270,  0.0790,  0.0945, -0.1582])\n",
      "\n",
      "W_ff2 (Feed forward layer 2 weights) shape: torch.Size([16, 4])\n",
      "tensor([[ 0.1400,  0.1187, -0.8132, -0.1839],\n",
      "        [ 0.4175,  0.0669,  0.2052, -0.3974],\n",
      "        [ 0.2432,  0.2264,  0.1244,  0.4099],\n",
      "        [ 0.4184, -0.2219, -0.1450, -0.2221],\n",
      "        [ 0.2484,  0.0331, -0.0905, -0.4210],\n",
      "        [-0.3981, -0.2984, -0.1482,  0.3410],\n",
      "        [ 0.1848, -0.0753,  0.2463, -0.0203],\n",
      "        [ 0.2847, -0.1195,  0.2070, -0.3939],\n",
      "        [-0.3365,  0.6301,  0.7099, -0.1897],\n",
      "        [-0.1085, -0.5075, -0.4152,  0.1461],\n",
      "        [ 0.0834,  0.0275,  0.4549, -0.1317],\n",
      "        [-0.1200, -0.3987,  0.4774, -0.0362],\n",
      "        [ 0.3368,  0.0294,  0.3368,  0.3747],\n",
      "        [ 0.4258,  0.0456,  0.3290, -0.2051],\n",
      "        [ 0.1978,  0.2010, -0.0378, -0.1838],\n",
      "        [-0.1451, -0.1626, -0.3179,  0.0917]])\n",
      "\n",
      "b_ff2 (Feed forward layer 2 bias) shape: torch.Size([4])\n",
      "tensor([ 0.0695, -0.2181, -0.0060, -0.0344])\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 16  # Dimension of the hidden layer in the feed-forward network which is typically 4 times d_model\n",
    "\n",
    "W_ff1 = torch.randn(d_model, hidden_dim) * 0.3    # Shape (d, hidden_dim) to expand from nxd_model to nxhidden_dim\n",
    "b_ff1 = torch.randn(hidden_dim) * 0.1 # Shape (hidden_dim,) bias for the first feed forward layer where it is broadcasted across the n tokens and added dimension wise\n",
    "W_ff2 = torch.randn(hidden_dim, d_model) * 0.3    # Shape (hidden_dim, d) to project back from nxhidden_dim to nxd_model\n",
    "b_ff2 = torch.randn(d_model) * 0.1 # Shape (d,) bias for the second feed forward layer where it is broadcasted across the n tokens and added dimension wise\n",
    "dropout_ff = nn.Dropout(p=0.2)\n",
    "\n",
    "print(f\"\\nW_ff1 (Feed forward layer 1 weights) shape: {W_ff1.shape}\")\n",
    "print(W_ff1)\n",
    "print(f\"\\nb_ff1 (Feed forward layer 1 bias) shape: {b_ff1.shape}\")\n",
    "print(b_ff1)\n",
    "print(f\"\\nW_ff2 (Feed forward layer 2 weights) shape: {W_ff2.shape}\")\n",
    "print(W_ff2)\n",
    "print(f\"\\nb_ff2 (Feed forward layer 2 bias) shape: {b_ff2.shape}\")\n",
    "print(b_ff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4df94296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForwardNetwork(layernorm_output, W_ff1, b_ff1, W_ff2, b_ff2):\n",
    "    ffn_layer1 = torch.matmul(layernorm_output, W_ff1) + b_ff1 # Shape (n, hidden_dim)\n",
    "    print(\"\\nFeed forward layer 1 output shape before ReLU:\", ffn_layer1.shape)\n",
    "    print(ffn_layer1)\n",
    "\n",
    "    ffn_layer1_activated = F.gelu(ffn_layer1) # Shape (n, hidden_dim). GeLU is applied element wise to the output of the first feed forward layer\n",
    "    print(\"\\nFeed forward layer 1 output shape after ReLU:\", ffn_layer1_activated.shape)\n",
    "    ffn_layer1_activated = dropout_ff(ffn_layer1_activated)  # Apply dropout to the activated output of the first feed forward layer\n",
    "    print(ffn_layer1_activated)\n",
    "\n",
    "    ffn_layer2 = torch.matmul(ffn_layer1_activated, W_ff2) + b_ff2 # Shape (n, d)\n",
    "    ffn_layer2 = dropout_ff(ffn_layer2)  # Apply dropout to the output of the second feed forward layer\n",
    "    print(\"\\nFeed forward layer 2 output shape:\", ffn_layer2.shape)\n",
    "    print(ffn_layer2)\n",
    "\n",
    "    return ffn_layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f98d206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feed forward layer 1 output shape before ReLU: torch.Size([9, 16])\n",
      "tensor([[ 0.9638, -0.2541,  0.0418,  0.3096, -0.6207,  0.0692,  0.1029, -0.1538,\n",
      "          0.1861,  0.5237, -1.4098, -0.0333,  0.0055,  0.5603,  0.1031, -0.3520],\n",
      "        [ 1.2587, -0.2484,  0.0670, -0.0203, -0.1197,  0.5975, -0.3917, -0.0414,\n",
      "         -0.2658,  0.2263, -0.9475,  0.0028,  0.0472, -0.0202,  0.7571,  0.4013],\n",
      "        [ 0.3669,  0.6019,  0.4164, -0.0518, -0.0367, -0.5535, -0.1912, -0.2018,\n",
      "         -0.6342, -0.4079, -0.1323,  0.1020, -0.3317,  0.7848, -0.6667, -2.0625],\n",
      "        [-0.2730, -0.1560, -0.0665,  0.6263, -0.7306, -0.6776,  0.7084, -0.2672,\n",
      "          0.6618,  0.5520, -0.9689, -0.0333, -0.0192,  0.9617, -0.7561, -1.1461],\n",
      "        [-1.1970,  0.3403,  0.0540,  0.2717,  0.1373, -0.8819,  0.4046, -0.2320,\n",
      "          0.0574, -0.2718,  0.6399,  0.0802, -0.1430,  0.6143, -0.9532, -1.5983],\n",
      "        [ 1.0548,  0.1251,  0.2631,  0.1775, -0.5704, -0.2099, -0.0307, -0.1917,\n",
      "         -0.1760,  0.2189, -1.2407,  0.0132, -0.1756,  0.8275, -0.2714, -1.2830],\n",
      "        [-0.7440, -0.5137, -0.3400,  0.6494, -0.4158, -0.3022,  0.7387, -0.1876,\n",
      "          0.9113,  0.6452, -0.5389, -0.0532,  0.2006,  0.3811, -0.2268,  0.1047],\n",
      "        [ 0.8731,  0.4488,  0.3892, -0.2845,  0.2543,  0.0553, -0.6011, -0.0932,\n",
      "         -0.9052, -0.4778, -0.0692,  0.1063, -0.2410,  0.2560,  0.0769, -1.0987],\n",
      "        [-0.7818,  0.4445,  0.1460, -0.1686,  0.7551, -0.2992, -0.2426, -0.1037,\n",
      "         -0.5950, -0.7102,  1.2262,  0.1358, -0.1402, -0.0237, -0.2416, -0.9164]])\n",
      "\n",
      "Feed forward layer 1 output shape after ReLU: torch.Size([9, 16])\n",
      "tensor([[ 1.0028, -0.1270,  0.0270,  0.2406, -0.2075,  0.0000,  0.0696, -0.0844,\n",
      "          0.0000,  0.4581, -0.1397, -0.0203,  0.0035,  0.4989,  0.0698, -0.1595],\n",
      "        [ 1.4097, -0.1248,  0.0441, -0.0125, -0.0677,  0.0000, -0.1702, -0.0000,\n",
      "         -0.1313,  0.1668, -0.2033,  0.0017,  0.0306, -0.0124,  0.7339,  0.3290],\n",
      "        [ 0.2949,  0.5465,  0.3443, -0.0000, -0.0223, -0.2006, -0.1014, -0.1060,\n",
      "         -0.2085, -0.1742, -0.0740,  0.0689, -0.1534,  0.7688, -0.2104, -0.0000],\n",
      "        [-0.1339, -0.0854, -0.0393,  0.5749, -0.2123, -0.2109,  0.6735, -0.0000,\n",
      "          0.6171,  0.4896, -0.2014, -0.0000, -0.0118,  1.0001, -0.2125, -0.1803],\n",
      "        [-0.1730,  0.2694,  0.0352,  0.2062,  0.0952, -0.2083,  0.3323, -0.0000,\n",
      "          0.0375, -0.1335,  0.5910,  0.0533, -0.0000,  0.5610, -0.2028, -0.0000],\n",
      "        [ 1.1264,  0.0860,  0.1985,  0.1266, -0.2026, -0.1094, -0.0000, -0.1016,\n",
      "         -0.0947,  0.1605, -0.1665,  0.0083, -0.0945,  0.8234, -0.1334, -0.0000],\n",
      "        [-0.2124, -0.1950, -0.1559,  0.0000, -0.1761, -0.1440,  0.7110, -0.0998,\n",
      "          0.9329,  0.5973, -0.1987, -0.0319,  0.1453,  0.3089, -0.1163,  0.0709],\n",
      "        [ 0.8826,  0.3777,  0.3170, -0.1380,  0.1908,  0.0361, -0.2058, -0.0000,\n",
      "         -0.2067, -0.1890, -0.0409,  0.0720, -0.1219,  0.1924,  0.0510, -0.0000],\n",
      "        [-0.2122,  0.3731,  0.1018, -0.0913,  0.7314, -0.1430, -0.1226, -0.0594,\n",
      "         -0.2052, -0.2120,  1.3640,  0.0940, -0.0000, -0.0146, -0.1222, -0.2059]])\n",
      "\n",
      "Feed forward layer 2 output shape: torch.Size([9, 4])\n",
      "tensor([[ 0.4914, -0.4029, -1.1384, -0.1675],\n",
      "        [ 0.3543, -0.1439, -1.9834, -0.0000],\n",
      "        [ 0.9460, -0.0000,  0.0334, -0.4189],\n",
      "        [ 0.6629, -0.2554,  0.9339, -0.4024],\n",
      "        [ 0.8300, -0.1837,  1.0465, -0.5537],\n",
      "        [ 0.7758, -0.2061, -1.0575, -0.3139],\n",
      "        [-0.0000, -0.0000,  0.9253,  0.0000],\n",
      "        [ 0.6176, -0.0294, -0.8654, -0.4128],\n",
      "        [ 0.7189, -0.1317,  0.0000, -0.7262]])\n"
     ]
    }
   ],
   "source": [
    "ffn_output = feedForwardNetwork(layernorm_output, W_ff1, b_ff1, W_ff2, b_ff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "31f4cbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final output shape: torch.Size([9, 4])\n",
      "tensor([[ 0.3518,  0.3761, -1.6811,  0.9532],\n",
      "        [ 0.0662,  1.0780, -1.6159,  0.4716],\n",
      "        [-1.2074, -0.7282,  1.2309,  0.7047],\n",
      "        [ 0.9624, -1.5468, -0.2131,  0.7974],\n",
      "        [ 0.4372, -1.4138,  1.3047, -0.3281],\n",
      "        [-0.0773,  0.3589, -1.5238,  1.2422],\n",
      "        [ 1.4181, -1.2953, -0.4509,  0.3281],\n",
      "        [-1.4351,  1.3783, -0.1117,  0.1685],\n",
      "        [ 0.1541, -0.5898,  1.5470, -1.1113]])\n"
     ]
    }
   ],
   "source": [
    "final_output = residualPlusLayerNorm(ffn_output, layernorm_output, gamma2, beta2)\n",
    "\n",
    "print(\"\\nFinal output shape:\", final_output.shape)\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5f7b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This completes one block of the transformer architecture (decoder). In practice, multiple such blocks are stacked to form a deep transformer model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
