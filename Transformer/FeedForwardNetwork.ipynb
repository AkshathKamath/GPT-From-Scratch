{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4172ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8370efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implemented using the Attention notebooks in the Attention directory\n",
    "\n",
    "def selfAttention(input_embeddings, W_q, W_k, W_v, W_o):\n",
    "    n = input_embeddings.shape[0]\n",
    "    d_model = input_embeddings.shape[1]\n",
    "    d_k = W_q.shape[1]\n",
    "\n",
    "    Q = torch.matmul(input_embeddings, W_q)\n",
    "    K = torch.matmul(input_embeddings, W_k)\n",
    "    V = torch.matmul(input_embeddings, W_v)\n",
    "\n",
    "    mask  = torch.tril(torch.ones(n, n))\n",
    "\n",
    "    attention_scores = torch.matmul(Q, K.T)\n",
    "    masked_attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "    masked_attention_scores /= torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "    attention_weights = F.softmax(masked_attention_scores, dim=-1)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    final_output = torch.matmul(output, W_o)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6648c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implemented using the LayerNorm notebook in the LayerNorm directory\n",
    "def residualPlusLayerNorm(attention_output, input_embeddings, gamma, beta, eps = 1e-5,):\n",
    "    residual_output = attention_output + input_embeddings\n",
    "\n",
    "    means = torch.mean(residual_output, dim=-1, keepdim=True) # Shape (n, 1)\n",
    "    variances = torch.var(residual_output, dim=-1, keepdim=True, unbiased=False) # Shape (n, 1)\n",
    "    normalized = (residual_output - means) / torch.sqrt(variances + eps) # Shape (n, d)\n",
    "\n",
    "    ln_output = normalized * gamma + beta\n",
    "    return ln_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb5b92c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "Number of tokens: 9\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "## Simple tokenization by splitting on spaces, ideally more complex tokenization would be used like BPE or WordPiece\n",
    "sentence = sentence.split()\n",
    "n = len(sentence)\n",
    "\n",
    "print(f\"Tokenized sentence: {sentence}\")\n",
    "print(f\"Number of tokens: {len(sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4dc1937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings (4-dimensional):\n",
      "  The : tensor([1.0000, 0.5000, 0.2000, 0.8000])\n",
      "  quick: tensor([0.3000, 1.0000, 0.7000, 0.1000])\n",
      "  brown: tensor([0.6000, 0.2000, 1.0000, 0.4000])\n",
      "  fox : tensor([0.9000, 0.8000, 0.3000, 1.0000])\n",
      "  jumps: tensor([0.4000, 0.6000, 0.8000, 0.2000])\n",
      "  over: tensor([0.7000, 0.3000, 0.5000, 0.9000])\n",
      "  the : tensor([1.0000, 0.5000, 0.2000, 0.8000])\n",
      "  lazy: tensor([0.2000, 0.9000, 0.4000, 0.6000])\n",
      "  dog : tensor([0.8000, 0.4000, 0.9000, 0.3000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Sample word embeddings, ideally these would be learned in the language modelling process or loaded from a pre-trained model like GloVe or Word2Vec\n",
    "\n",
    "# Shape of embeddings: (n, d) where n is number of tokens and d is embedding dimension\n",
    "embeddings = torch.tensor([\n",
    "        [1.0, 0.5, 0.2, 0.8], \n",
    "        [0.3, 1.0, 0.7, 0.1],  \n",
    "        [0.6, 0.2, 1.0, 0.4],  \n",
    "        [0.9, 0.8, 0.3, 1.0],  \n",
    "        [0.4, 0.6, 0.8, 0.2],  \n",
    "        [0.7, 0.3, 0.5, 0.9],  \n",
    "        [1.0, 0.5, 0.2, 0.8],  \n",
    "        [0.2, 0.9, 0.4, 0.6],  \n",
    "        [0.8, 0.4, 0.9, 0.3]  \n",
    "    ])\n",
    "\n",
    "print(\"Word embeddings (4-dimensional):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  {word:4}: {embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26ec4ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embeddings (Same dimesnions as word embeddings):\n",
      "  Pos 0 (The): tensor([0., 1., 0., 1.])\n",
      "  Pos 1 (quick): tensor([0.1000, 0.9000, 0.1000, 0.9000])\n",
      "  Pos 2 (brown): tensor([0.2000, 0.8000, 0.2000, 0.8000])\n",
      "  Pos 3 (fox): tensor([0.3000, 0.7000, 0.3000, 0.7000])\n",
      "  Pos 4 (jumps): tensor([0.4000, 0.6000, 0.4000, 0.6000])\n",
      "  Pos 5 (over): tensor([0.5000, 0.5000, 0.5000, 0.5000])\n",
      "  Pos 6 (the): tensor([0.6000, 0.4000, 0.6000, 0.4000])\n",
      "  Pos 7 (lazy): tensor([0.7000, 0.3000, 0.7000, 0.3000])\n",
      "  Pos 8 (dog): tensor([0.8000, 0.2000, 0.8000, 0.2000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Sample positional encodings, typically these would be generated using math functions or learned during training or RoPE\n",
    "\n",
    "positional_embeddings = torch.tensor([\n",
    "    [0.0, 1.0, 0.0, 1.0],  \n",
    "    [0.1, 0.9, 0.1, 0.9],  \n",
    "    [0.2, 0.8, 0.2, 0.8],  \n",
    "    [0.3, 0.7, 0.3, 0.7],  \n",
    "    [0.4, 0.6, 0.4, 0.6],  \n",
    "    [0.5, 0.5, 0.5, 0.5],  \n",
    "    [0.6, 0.4, 0.6, 0.4],  \n",
    "    [0.7, 0.3, 0.7, 0.3],  \n",
    "    [0.8, 0.2, 0.8, 0.2]   \n",
    "])\n",
    "\n",
    "print(\"Positional embeddings (Same dimesnions as word embeddings):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  Pos {i} ({word}): {positional_embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e7ec4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings (word + positional):\n",
      "  The : tensor([1.0000, 1.5000, 0.2000, 1.8000])\n",
      "  quick: tensor([0.4000, 1.9000, 0.8000, 1.0000])\n",
      "  brown: tensor([0.8000, 1.0000, 1.2000, 1.2000])\n",
      "  fox : tensor([1.2000, 1.5000, 0.6000, 1.7000])\n",
      "  jumps: tensor([0.8000, 1.2000, 1.2000, 0.8000])\n",
      "  over: tensor([1.2000, 0.8000, 1.0000, 1.4000])\n",
      "  the : tensor([1.6000, 0.9000, 0.8000, 1.2000])\n",
      "  lazy: tensor([0.9000, 1.2000, 1.1000, 0.9000])\n",
      "  dog : tensor([1.6000, 0.6000, 1.7000, 0.5000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## The final input to the Attention block is the sum of the word embeddings and positional encodings\n",
    "\n",
    "input_embeddings = embeddings + positional_embeddings\n",
    "\n",
    "print(\"Input embeddings (word + positional):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  {word:4}: {input_embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a342beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = input_embeddings.shape[1]  # Embedding dimension\n",
    "d_k = 3 # Dimension of keys and queries (generally kept smaller to make Q, K and V matrices low rank for efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f25bee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_q (Query weights) shape: torch.Size([4, 3])\n",
      "tensor([[ 0.1010,  0.0386,  0.0703],\n",
      "        [ 0.0691, -0.3369, -0.0559],\n",
      "        [ 0.6625, -0.1914,  0.1385],\n",
      "        [ 0.0802,  0.1605,  0.2428]])\n",
      "\n",
      "W_k (Key weights) shape: torch.Size([4, 3])\n",
      "tensor([[ 0.3331, -0.5069, -0.2967],\n",
      "        [ 0.2874,  0.3966,  0.2452],\n",
      "        [-0.2298, -0.2252,  0.4058],\n",
      "        [ 0.2059, -0.0983,  0.2385]])\n",
      "\n",
      "W_v (Value weights) shape: torch.Size([4, 3])\n",
      "tensor([[ 0.0845,  0.0168,  0.1568],\n",
      "        [-0.0715, -0.0150,  0.1579],\n",
      "        [-0.0025,  0.2187,  0.0399],\n",
      "        [ 0.2592, -0.3047, -0.2666]])\n",
      "\n",
      "W_o (Output projection weights) shape: torch.Size([3, 4])\n",
      "tensor([[ 0.0449, -0.0627, -0.1161,  0.2974],\n",
      "        [ 0.1404, -0.0615, -0.2223,  0.1086],\n",
      "        [ 0.5760, -0.0676, -0.1025,  0.0912]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)  # For reproducible results\n",
    "\n",
    "## Shape of the Q, K and V matrices is d x d_k and for the output projection matrix is d_k x d to project the attention output back to d dimensions\n",
    "W_q = torch.randn(d_model, d_k, dtype=torch.float32) * 0.3  \n",
    "W_k = torch.randn(d_model, d_k, dtype=torch.float32) * 0.3  \n",
    "W_v = torch.randn(d_model, d_k, dtype=torch.float32) * 0.3  \n",
    "W_o = torch.randn(d_k, d_model, dtype=torch.float32) * 0.3 \n",
    "\n",
    "print(f\"W_q (Query weights) shape: {W_q.shape}\")\n",
    "print(W_q)\n",
    "print(f\"\\nW_k (Key weights) shape: {W_k.shape}\")\n",
    "print(W_k)\n",
    "print(f\"\\nW_v (Value weights) shape: {W_v.shape}\")\n",
    "print(W_v)\n",
    "print()\n",
    "\n",
    "print(f\"W_o (Output projection weights) shape: {W_o.shape}\")\n",
    "print(W_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "462aeb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention output shape: torch.Size([9, 4])\n",
      "tensor([[-0.0968,  0.0089,  0.0700,  0.0693],\n",
      "        [-0.0290,  0.0011,  0.0407,  0.0572],\n",
      "        [-0.0174, -0.0035,  0.0238,  0.0640],\n",
      "        [-0.0211, -0.0036,  0.0262,  0.0700],\n",
      "        [-0.0030, -0.0066,  0.0158,  0.0706],\n",
      "        [-0.0044, -0.0076,  0.0127,  0.0743],\n",
      "        [ 0.0043, -0.0094,  0.0083,  0.0785],\n",
      "        [ 0.0124, -0.0108,  0.0039,  0.0788],\n",
      "        [ 0.0287, -0.0139, -0.0053,  0.0823]])\n"
     ]
    }
   ],
   "source": [
    "attention_output = selfAttention(input_embeddings, W_q, W_k, W_v, W_o)\n",
    "\n",
    "print(\"\\nAttention output shape:\", attention_output.shape)\n",
    "print(attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "881a1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The parameters for the layer norm layer that will be applied after the attention layer\n",
    "gamma1 = torch.ones(d_model, dtype=torch.float32)\n",
    "beta1 = torch.zeros(d_model, dtype=torch.float32)\n",
    "\n",
    "## The parameters for the layer norm layer that will be applied after the feed forward layer\n",
    "gamma2 = torch.ones(d_model, dtype=torch.float32)\n",
    "beta2 = torch.zeros(d_model, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f5fe6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LayerNorm output shape: torch.Size([9, 4])\n",
      "tensor([[-0.3856,  0.6098, -1.4263,  1.2021],\n",
      "        [-1.2114,  1.5489, -0.3639,  0.0265],\n",
      "        [-1.4708, -0.3633,  0.8130,  1.0211],\n",
      "        [-0.2092,  0.5371, -1.5082,  1.1802],\n",
      "        [-1.1857,  0.9293,  1.0492, -0.7928],\n",
      "        [ 0.3073, -1.3051, -0.4240,  1.4219],\n",
      "        [ 1.4387, -0.7991, -1.0569,  0.4173],\n",
      "        [-1.2430,  1.3310,  0.5375, -0.6255],\n",
      "        [ 0.9378, -0.9955,  1.0603, -1.0026]])\n"
     ]
    }
   ],
   "source": [
    "layernorm_output = residualPlusLayerNorm(attention_output, input_embeddings, gamma1, beta1)\n",
    "\n",
    "print(\"\\nLayerNorm output shape:\", layernorm_output.shape)\n",
    "print(layernorm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db48102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W_ff1 (Feed forward layer 1 weights) shape: torch.Size([4, 16])\n",
      "tensor([[-7.5286e-01,  1.4640e-01,  2.3538e-01,  8.5942e-03,  1.9223e-01,\n",
      "          1.7497e-01,  3.2008e-01, -1.3505e-01, -5.5580e-02,  2.2583e-01,\n",
      "          1.2143e-01,  5.3540e-02,  7.9473e-02,  3.8195e-01, -3.9326e-04,\n",
      "         -9.1081e-02],\n",
      "        [-4.3711e-01, -3.0701e-02, -1.7975e-01,  1.4312e-01,  2.1785e-01,\n",
      "          2.7346e-02, -1.1672e-01,  1.5837e-01, -3.8056e-03,  7.2251e-02,\n",
      "          3.9761e-02,  2.2927e-01,  3.2850e-01,  1.0197e-01,  2.1599e-01,\n",
      "          1.2342e-01],\n",
      "        [ 5.7935e-01,  3.0356e-01, -4.3092e-01, -3.3896e-01, -4.0810e-02,\n",
      "          4.9062e-01,  1.9642e-01,  1.7280e-01,  3.4245e-01,  5.5694e-03,\n",
      "         -5.4174e-01,  2.7763e-01, -1.1260e-01,  3.0993e-01, -2.0600e-01,\n",
      "          1.9104e-01],\n",
      "        [-2.9180e-01,  2.8754e-01,  4.8576e-01,  4.3518e-01,  8.0844e-02,\n",
      "         -6.3113e-02, -2.1984e-01,  3.1289e-02,  1.0463e-01,  2.9028e-01,\n",
      "         -1.3971e-01,  4.8144e-01, -7.4404e-01, -1.2526e-01, -3.5864e-01,\n",
      "          2.4370e-01]])\n",
      "\n",
      "b_ff1 (Feed forward layer 1 bias) shape: torch.Size([16])\n",
      "tensor([-0.1901,  0.0229,  0.0025, -0.0346,  0.0287, -0.0731,  0.0175, -0.1094,\n",
      "        -0.1602,  0.1353,  0.1289,  0.0052, -0.1547,  0.0757,  0.0776,  0.2027])\n",
      "\n",
      "W_ff2 (Feed forward layer 2 weights) shape: torch.Size([16, 4])\n",
      "tensor([[ 0.0107,  0.0362, -0.2417, -0.0623],\n",
      "        [-0.2796, -0.4773, -0.3408, -0.1568],\n",
      "        [-0.1556, -0.4504, -0.5780,  0.0384],\n",
      "        [ 0.3069, -0.1667,  0.2113,  0.2130],\n",
      "        [ 0.5323, -0.2765,  0.2887, -0.1011],\n",
      "        [-0.3526,  0.1074,  0.1436,  0.4061],\n",
      "        [ 0.1578,  0.6336, -0.1562, -0.2796],\n",
      "        [ 0.0555,  0.3206,  0.3920,  0.1380],\n",
      "        [-0.2444, -0.3064, -0.1485, -0.1777],\n",
      "        [ 0.0463,  0.1322, -0.0445, -0.6955],\n",
      "        [-0.1194,  0.3241, -0.5343,  0.4524],\n",
      "        [ 0.0928, -0.1501,  0.3105,  0.5069],\n",
      "        [-0.0014,  0.5000,  0.0462, -0.3181],\n",
      "        [-0.1718,  0.0251,  0.1200,  0.5968],\n",
      "        [-0.0216, -0.2718, -0.6146, -0.3243],\n",
      "        [ 0.0053,  0.0235,  0.0579,  0.1229]])\n",
      "\n",
      "b_ff2 (Feed forward layer 2 bias) shape: torch.Size([4])\n",
      "tensor([-0.0428, -0.0501, -0.0131,  0.0980])\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 16  # Dimension of the hidden layer in the feed-forward network which is typically 4 times d_model\n",
    "\n",
    "W_ff1 = torch.randn(d_model, hidden_dim) * 0.3    # Shape (d, hidden_dim) to expand from nxd_model to nxhidden_dim\n",
    "b_ff1 = torch.randn(hidden_dim) * 0.1 # Shape (hidden_dim,) bias for the first feed forward layer where it is broadcasted across the n tokens and added dimension wise\n",
    "W_ff2 = torch.randn(hidden_dim, d_model) * 0.3    # Shape (hidden_dim, d) to project back from nxhidden_dim to nxd_model\n",
    "b_ff2 = torch.randn(d_model) * 0.1 # Shape (d,) bias for the second feed forward layer where it is broadcasted across the n tokens and added dimension wise\n",
    "\n",
    "print(f\"\\nW_ff1 (Feed forward layer 1 weights) shape: {W_ff1.shape}\")\n",
    "print(W_ff1)\n",
    "print(f\"\\nb_ff1 (Feed forward layer 1 bias) shape: {b_ff1.shape}\")\n",
    "print(b_ff1)\n",
    "print(f\"\\nW_ff2 (Feed forward layer 2 weights) shape: {W_ff2.shape}\")\n",
    "print(W_ff2)\n",
    "print(f\"\\nb_ff2 (Feed forward layer 2 bias) shape: {b_ff2.shape}\")\n",
    "print(b_ff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df94296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForwardNetwork(layernorm_output, W_ff1, b_ff1, W_ff2, b_ff2):\n",
    "    ffn_layer1 = torch.matmul(layernorm_output, W_ff1) + b_ff1 # Shape (n, hidden_dim)\n",
    "    print(\"\\nFeed forward layer 1 output shape before ReLU:\", ffn_layer1.shape)\n",
    "    print(ffn_layer1)\n",
    "\n",
    "    ffn_layer1_activated = F.relu(ffn_layer1) # Shape (n, hidden_dim). ReLU is applied element wise to the output of the first feed forward layer\n",
    "    print(\"\\nFeed forward layer 1 output shape after ReLU:\", ffn_layer1_activated.shape)\n",
    "    print(ffn_layer1_activated)\n",
    "\n",
    "    ffn_layer2 = torch.matmul(ffn_layer1_activated, W_ff2) + b_ff2 # Shape (n, d)\n",
    "    print(\"\\nFeed forward layer 2 output shape:\", ffn_layer2.shape)\n",
    "    print(ffn_layer2)\n",
    "\n",
    "    return ffn_layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f98d206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feed forward layer 1 output shape before ReLU: torch.Size([9, 16])\n",
      "tensor([[-1.3434, -0.1396,  1.0007,  1.0559,  0.2428, -0.8995, -0.7215, -0.1696,\n",
      "         -0.5038,  0.4333,  0.7110,  0.3072, -0.7188, -0.6021,  0.0721,  0.3335],\n",
      "        [-0.1736, -0.3049, -0.3914,  0.3116,  0.1502, -0.4229, -0.6284,  0.2375,\n",
      "         -0.2206, -0.0207,  0.2368,  0.2072,  0.2791, -0.3452,  0.4781,  0.4411],\n",
      "        [ 1.2491,  0.3591, -0.1327,  0.0696, -0.2838, -0.0059, -0.4757,  0.2041,\n",
      "          0.3082,  0.0778, -0.6473,  0.5605, -1.2422, -0.3991, -0.5340,  0.6959],\n",
      "        [-1.4855, -0.1427,  1.0799,  1.0653,  0.2625, -0.9094, -0.6679, -0.2198,\n",
      "         -0.5436,  0.4611,  0.7770,  0.2667, -0.7032, -0.5647,  0.0811,  0.2875],\n",
      "        [ 1.1356, -0.0887, -1.2809, -0.6124, -0.1037,  0.3096, -0.0902,  0.3544,\n",
      "          0.1785, -0.2896, -0.4358,  0.0644,  0.5281,  0.1420,  0.3469,  0.4326],\n",
      "        [-0.5115,  0.3880,  1.1828,  0.5438, -0.0643, -0.3528, -0.1277, -0.3864,\n",
      "         -0.1688,  0.5208,  0.1454,  0.2893, -1.5692, -0.2496, -0.6270,  0.2791],\n",
      "        [-1.6580,  0.0572,  1.1429,  0.4033,  0.2080, -0.3881,  0.2719, -0.5998,\n",
      "         -0.5554,  0.5177,  0.7861, -0.1935, -0.4943,  0.1639, -0.0276, -0.1272],\n",
      "        [ 0.6579, -0.2167, -1.0648, -0.3092,  0.0072,  0.0490, -0.2926,  0.3426,\n",
      "          0.0224, -0.2278, -0.1729,  0.0919,  0.5886, -0.0184,  0.4791,  0.4304],\n",
      "        [ 0.4458,  0.2243, -0.5418, -0.9647, -0.1322,  0.6473,  0.8625, -0.2419,\n",
      "          0.0496, -0.0100, -0.2311, -0.3611,  0.2194,  0.7866,  0.0033, -0.0474]])\n",
      "\n",
      "Feed forward layer 1 output shape after ReLU: torch.Size([9, 16])\n",
      "tensor([[0.0000, 0.0000, 1.0007, 1.0559, 0.2428, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.4333, 0.7110, 0.3072, 0.0000, 0.0000, 0.0721, 0.3335],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3116, 0.1502, 0.0000, 0.0000, 0.2375, 0.0000,\n",
      "         0.0000, 0.2368, 0.2072, 0.2791, 0.0000, 0.4781, 0.4411],\n",
      "        [1.2491, 0.3591, 0.0000, 0.0696, 0.0000, 0.0000, 0.0000, 0.2041, 0.3082,\n",
      "         0.0778, 0.0000, 0.5605, 0.0000, 0.0000, 0.0000, 0.6959],\n",
      "        [0.0000, 0.0000, 1.0799, 1.0653, 0.2625, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.4611, 0.7770, 0.2667, 0.0000, 0.0000, 0.0811, 0.2875],\n",
      "        [1.1356, 0.0000, 0.0000, 0.0000, 0.0000, 0.3096, 0.0000, 0.3544, 0.1785,\n",
      "         0.0000, 0.0000, 0.0644, 0.5281, 0.1420, 0.3469, 0.4326],\n",
      "        [0.0000, 0.3880, 1.1828, 0.5438, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.5208, 0.1454, 0.2893, 0.0000, 0.0000, 0.0000, 0.2791],\n",
      "        [0.0000, 0.0572, 1.1429, 0.4033, 0.2080, 0.0000, 0.2719, 0.0000, 0.0000,\n",
      "         0.5177, 0.7861, 0.0000, 0.0000, 0.1639, 0.0000, 0.0000],\n",
      "        [0.6579, 0.0000, 0.0000, 0.0000, 0.0072, 0.0490, 0.0000, 0.3426, 0.0224,\n",
      "         0.0000, 0.0000, 0.0919, 0.5886, 0.0000, 0.4791, 0.4304],\n",
      "        [0.4458, 0.2243, 0.0000, 0.0000, 0.0000, 0.6473, 0.8625, 0.0000, 0.0496,\n",
      "         0.0000, 0.0000, 0.0000, 0.2194, 0.7866, 0.0033, 0.0000]])\n",
      "\n",
      "Feed forward layer 2 output shape: torch.Size([9, 4])\n",
      "tensor([[ 0.2186, -0.5140, -0.6270,  0.5303],\n",
      "        [ 0.1286, -0.0018, -0.1284,  0.2044],\n",
      "        [-0.1131, -0.2743, -0.1775,  0.2676],\n",
      "        [ 0.2089, -0.5291, -0.7224,  0.5148],\n",
      "        [-0.1880,  0.2570, -0.2574,  0.0603],\n",
      "        [-0.1334, -0.7795, -0.7089,  0.0828],\n",
      "        [-0.0573, -0.2172, -1.0137,  0.2150],\n",
      "        [-0.0359,  0.2404, -0.2458, -0.1237],\n",
      "        [-0.3404,  0.5884, -0.1440,  0.4464]])\n"
     ]
    }
   ],
   "source": [
    "ffn_output = feedForwardNetwork(layernorm_output, W_ff1, b_ff1, W_ff2, b_ff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31f4cbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final output shape: torch.Size([9, 4])\n",
      "tensor([[-0.0513,  0.1443, -1.4558,  1.3628],\n",
      "        [-1.1552,  1.5250, -0.5534,  0.1836],\n",
      "        [-1.3559, -0.5060,  0.6376,  1.2243],\n",
      "        [ 0.0944,  0.1004, -1.5049,  1.3101],\n",
      "        [-1.2715,  1.1546,  0.7807, -0.6638],\n",
      "        [ 0.4127, -1.2556, -0.5527,  1.3956],\n",
      "        [ 1.2178, -0.5522, -1.3304,  0.6648],\n",
      "        [-1.1364,  1.4807,  0.3057, -0.6500],\n",
      "        [ 0.7280, -0.8624,  1.2328, -1.0984]])\n"
     ]
    }
   ],
   "source": [
    "final_output = residualPlusLayerNorm(ffn_output, layernorm_output, gamma2, beta2)\n",
    "\n",
    "print(\"\\nFinal output shape:\", final_output.shape)\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5f7b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This completes one block of the transformer architecture (decoder). In practice, multiple such blocks are stacked to form a deep transformer model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
