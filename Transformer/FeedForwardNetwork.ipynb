{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4172ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8370efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implemented using the Attention notebooks in the Attention directory\n",
    "\n",
    "def selfAttention(input_embeddings, W_q, W_k, W_v, W_o):\n",
    "    n = input_embeddings.shape[0]\n",
    "    d_model = input_embeddings.shape[1]\n",
    "    d_k = W_q.shape[1]\n",
    "\n",
    "    Q = torch.matmul(input_embeddings, W_q)\n",
    "    K = torch.matmul(input_embeddings, W_k)\n",
    "    V = torch.matmul(input_embeddings, W_v)\n",
    "    dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    mask  = torch.tril(torch.ones(n, n))\n",
    "\n",
    "    attention_scores = torch.matmul(Q, K.T)\n",
    "    masked_attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "    masked_attention_scores /= torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "    attention_weights = F.softmax(masked_attention_scores, dim=-1)\n",
    "    attention_weights = dropout(attention_weights)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    final_output = torch.matmul(output, W_o)\n",
    "    final_output = dropout(final_output)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6648c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implemented using the LayerNorm notebook in the LayerNorm directory\n",
    "def residualPlusLayerNorm(attention_output, input_embeddings, gamma, beta, eps = 1e-5,):\n",
    "    residual_output = attention_output + input_embeddings\n",
    "\n",
    "    means = torch.mean(residual_output, dim=-1, keepdim=True) # Shape (n, 1)\n",
    "    variances = torch.var(residual_output, dim=-1, keepdim=True, unbiased=False) # Shape (n, 1)\n",
    "    normalized = (residual_output - means) / torch.sqrt(variances + eps) # Shape (n, d)\n",
    "\n",
    "    ln_output = normalized * gamma + beta\n",
    "    return ln_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb5b92c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "Number of tokens: 9\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "## Simple tokenization by splitting on spaces, ideally more complex tokenization would be used like BPE or WordPiece\n",
    "sentence = sentence.split()\n",
    "n = len(sentence)\n",
    "\n",
    "print(f\"Tokenized sentence: {sentence}\")\n",
    "print(f\"Number of tokens: {len(sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4dc1937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings (4-dimensional):\n",
      "  The : tensor([1.0000, 0.5000, 0.2000, 0.8000])\n",
      "  quick: tensor([0.3000, 1.0000, 0.7000, 0.1000])\n",
      "  brown: tensor([0.6000, 0.2000, 1.0000, 0.4000])\n",
      "  fox : tensor([0.9000, 0.8000, 0.3000, 1.0000])\n",
      "  jumps: tensor([0.4000, 0.6000, 0.8000, 0.2000])\n",
      "  over: tensor([0.7000, 0.3000, 0.5000, 0.9000])\n",
      "  the : tensor([1.0000, 0.5000, 0.2000, 0.8000])\n",
      "  lazy: tensor([0.2000, 0.9000, 0.4000, 0.6000])\n",
      "  dog : tensor([0.8000, 0.4000, 0.9000, 0.3000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Sample word embeddings, ideally these would be learned in the language modelling process or loaded from a pre-trained model like GloVe or Word2Vec\n",
    "\n",
    "# Shape of embeddings: (n, d) where n is number of tokens and d is embedding dimension\n",
    "embeddings = torch.tensor([\n",
    "        [1.0, 0.5, 0.2, 0.8], \n",
    "        [0.3, 1.0, 0.7, 0.1],  \n",
    "        [0.6, 0.2, 1.0, 0.4],  \n",
    "        [0.9, 0.8, 0.3, 1.0],  \n",
    "        [0.4, 0.6, 0.8, 0.2],  \n",
    "        [0.7, 0.3, 0.5, 0.9],  \n",
    "        [1.0, 0.5, 0.2, 0.8],  \n",
    "        [0.2, 0.9, 0.4, 0.6],  \n",
    "        [0.8, 0.4, 0.9, 0.3]  \n",
    "    ])\n",
    "\n",
    "print(\"Word embeddings (4-dimensional):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  {word:4}: {embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26ec4ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embeddings (Same dimesnions as word embeddings):\n",
      "  Pos 0 (The): tensor([0., 1., 0., 1.])\n",
      "  Pos 1 (quick): tensor([0.1000, 0.9000, 0.1000, 0.9000])\n",
      "  Pos 2 (brown): tensor([0.2000, 0.8000, 0.2000, 0.8000])\n",
      "  Pos 3 (fox): tensor([0.3000, 0.7000, 0.3000, 0.7000])\n",
      "  Pos 4 (jumps): tensor([0.4000, 0.6000, 0.4000, 0.6000])\n",
      "  Pos 5 (over): tensor([0.5000, 0.5000, 0.5000, 0.5000])\n",
      "  Pos 6 (the): tensor([0.6000, 0.4000, 0.6000, 0.4000])\n",
      "  Pos 7 (lazy): tensor([0.7000, 0.3000, 0.7000, 0.3000])\n",
      "  Pos 8 (dog): tensor([0.8000, 0.2000, 0.8000, 0.2000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Sample positional encodings, typically these would be generated using math functions or learned during training or RoPE\n",
    "\n",
    "positional_embeddings = torch.tensor([\n",
    "    [0.0, 1.0, 0.0, 1.0],  \n",
    "    [0.1, 0.9, 0.1, 0.9],  \n",
    "    [0.2, 0.8, 0.2, 0.8],  \n",
    "    [0.3, 0.7, 0.3, 0.7],  \n",
    "    [0.4, 0.6, 0.4, 0.6],  \n",
    "    [0.5, 0.5, 0.5, 0.5],  \n",
    "    [0.6, 0.4, 0.6, 0.4],  \n",
    "    [0.7, 0.3, 0.7, 0.3],  \n",
    "    [0.8, 0.2, 0.8, 0.2]   \n",
    "])\n",
    "\n",
    "print(\"Positional embeddings (Same dimesnions as word embeddings):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  Pos {i} ({word}): {positional_embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e7ec4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings (word + positional):\n",
      "  The : tensor([1.2500, 0.0000, 0.0000, 2.2500])\n",
      "  quick: tensor([0.5000, 2.3750, 0.0000, 1.2500])\n",
      "  brown: tensor([0.0000, 1.2500, 1.5000, 1.5000])\n",
      "  fox : tensor([1.5000, 1.8750, 0.7500, 2.1250])\n",
      "  jumps: tensor([1.0000, 1.5000, 1.5000, 1.0000])\n",
      "  over: tensor([1.5000, 1.0000, 1.2500, 1.7500])\n",
      "  the : tensor([2.0000, 1.1250, 1.0000, 1.5000])\n",
      "  lazy: tensor([1.1250, 1.5000, 1.3750, 1.1250])\n",
      "  dog : tensor([2.0000, 0.7500, 2.1250, 0.0000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## The final input to the Attention block is the sum of the word embeddings and positional encodings\n",
    "dropout = nn.Dropout(p=0.2)\n",
    "input_embeddings = embeddings + positional_embeddings\n",
    "input_embeddings = dropout(input_embeddings)\n",
    "\n",
    "print(\"Input embeddings (word + positional):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  {word:4}: {input_embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a342beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = input_embeddings.shape[1]  # Embedding dimension\n",
    "d_k = 3 # Dimension of keys and queries (generally kept smaller to make Q, K and V matrices low rank for efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f25bee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_q (Query weights) shape: torch.Size([4, 3])\n",
      "tensor([[ 0.1010,  0.0386,  0.0703],\n",
      "        [ 0.0691, -0.3369, -0.0559],\n",
      "        [ 0.6625, -0.1914,  0.1385],\n",
      "        [ 0.0802,  0.1605,  0.2428]])\n",
      "\n",
      "W_k (Key weights) shape: torch.Size([4, 3])\n",
      "tensor([[ 0.3331, -0.5069, -0.2967],\n",
      "        [ 0.2874,  0.3966,  0.2452],\n",
      "        [-0.2298, -0.2252,  0.4058],\n",
      "        [ 0.2059, -0.0983,  0.2385]])\n",
      "\n",
      "W_v (Value weights) shape: torch.Size([4, 3])\n",
      "tensor([[ 0.0845,  0.0168,  0.1568],\n",
      "        [-0.0715, -0.0150,  0.1579],\n",
      "        [-0.0025,  0.2187,  0.0399],\n",
      "        [ 0.2592, -0.3047, -0.2666]])\n",
      "\n",
      "W_o (Output projection weights) shape: torch.Size([3, 4])\n",
      "tensor([[ 0.0449, -0.0627, -0.1161,  0.2974],\n",
      "        [ 0.1404, -0.0615, -0.2223,  0.1086],\n",
      "        [ 0.5760, -0.0676, -0.1025,  0.0912]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)  # For reproducible results\n",
    "\n",
    "## Shape of the Q, K and V matrices is d x d_k and for the output projection matrix is d_k x d to project the attention output back to d dimensions\n",
    "W_q = torch.randn(d_model, d_k, dtype=torch.float32) * 0.3  \n",
    "W_k = torch.randn(d_model, d_k, dtype=torch.float32) * 0.3  \n",
    "W_v = torch.randn(d_model, d_k, dtype=torch.float32) * 0.3  \n",
    "W_o = torch.randn(d_k, d_model, dtype=torch.float32) * 0.3 \n",
    "\n",
    "print(f\"W_q (Query weights) shape: {W_q.shape}\")\n",
    "print(W_q)\n",
    "print(f\"\\nW_k (Key weights) shape: {W_k.shape}\")\n",
    "print(W_k)\n",
    "print(f\"\\nW_v (Value weights) shape: {W_v.shape}\")\n",
    "print(W_v)\n",
    "print()\n",
    "\n",
    "print(f\"W_o (Output projection weights) shape: {W_o.shape}\")\n",
    "print(W_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "462aeb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention output shape: torch.Size([9, 4])\n",
      "tensor([[-0.4609,  0.0000,  0.0000,  0.1498],\n",
      "        [-0.2590,  0.0261,  0.0000,  0.1045],\n",
      "        [-0.2106,  0.0147,  0.0693,  0.0813],\n",
      "        [-0.1512,  0.0088,  0.0871,  0.1181],\n",
      "        [-0.0072, -0.0095,  0.0098,  0.0866],\n",
      "        [-0.0805, -0.0063,  0.0318,  0.1259],\n",
      "        [ 0.0090, -0.0152,  0.0132,  0.1248],\n",
      "        [-0.0414, -0.0062,  0.0311,  0.1051],\n",
      "        [ 0.0000, -0.0181,  0.0077,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "attention_output = selfAttention(input_embeddings, W_q, W_k, W_v, W_o)\n",
    "\n",
    "print(\"\\nAttention output shape:\", attention_output.shape)\n",
    "print(attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "881a1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The parameters for the layer norm layer that will be applied after the attention layer\n",
    "gamma1 = torch.ones(d_model, dtype=torch.float32)\n",
    "beta1 = torch.zeros(d_model, dtype=torch.float32)\n",
    "\n",
    "## The parameters for the layer norm layer that will be applied after the feed forward layer\n",
    "gamma2 = torch.ones(d_model, dtype=torch.float32)\n",
    "beta2 = torch.zeros(d_model, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f5fe6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LayerNorm output shape: torch.Size([9, 4])\n",
      "tensor([[-0.0083, -0.8137, -0.8137,  1.6357],\n",
      "        [-0.7921,  1.4646, -1.0438,  0.3713],\n",
      "        [-1.7063,  0.2887,  0.7007,  0.7169],\n",
      "        [-0.4302,  0.5731, -1.3898,  1.2469],\n",
      "        [-1.1908,  0.9478,  1.0308, -0.7877],\n",
      "        [ 0.0840, -1.2528, -0.3483,  1.5171],\n",
      "        [ 1.4144, -0.8176, -1.0575,  0.4608],\n",
      "        [-1.3864,  1.2012,  0.6478, -0.4625],\n",
      "        [ 0.8808, -0.5442,  1.0299, -1.3665]])\n"
     ]
    }
   ],
   "source": [
    "layernorm_output = residualPlusLayerNorm(attention_output, input_embeddings, gamma1, beta1)\n",
    "\n",
    "print(\"\\nLayerNorm output shape:\", layernorm_output.shape)\n",
    "print(layernorm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db48102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W_ff1 (Feed forward layer 1 weights) shape: torch.Size([4, 16])\n",
      "tensor([[-0.0667, -0.3741, -0.1459, -0.1008,  0.0110,  0.1480,  0.2656,  0.0547,\n",
      "          0.5679,  0.1334,  0.0409,  0.0926,  0.4985,  0.0525,  0.1825,  0.4893],\n",
      "        [ 0.8556, -0.2231,  0.0586, -0.4005,  0.1184,  0.5118, -0.2382,  0.1126,\n",
      "          0.0229, -0.0638, -0.1699,  0.1197,  0.4108, -0.0756,  0.5701,  0.5085],\n",
      "        [ 0.0917,  0.0873,  0.1226, -0.3783,  0.2750, -0.0084, -0.0655,  0.0499,\n",
      "         -0.0261, -0.3541,  0.4638,  0.1634,  0.2980,  0.1520, -0.0419, -0.3542],\n",
      "        [ 0.5943, -0.0314,  0.1471, -0.1312, -0.3660, -0.1756,  0.1999, -0.0223,\n",
      "          0.2980,  0.0809, -0.5495,  0.1071,  0.2743,  0.6565, -0.2955, -0.7465]])\n",
      "\n",
      "b_ff1 (Feed forward layer 1 bias) shape: torch.Size([16])\n",
      "tensor([-0.1424, -0.0696, -0.0318,  0.1215,  0.1420, -0.0055,  0.0025, -0.1064,\n",
      "        -0.0364, -0.0099,  0.0311,  0.0371,  0.0270,  0.0790,  0.0945, -0.1582])\n",
      "\n",
      "W_ff2 (Feed forward layer 2 weights) shape: torch.Size([16, 4])\n",
      "tensor([[ 0.1400,  0.1187, -0.8132, -0.1839],\n",
      "        [ 0.4175,  0.0669,  0.2052, -0.3974],\n",
      "        [ 0.2432,  0.2264,  0.1244,  0.4099],\n",
      "        [ 0.4184, -0.2219, -0.1450, -0.2221],\n",
      "        [ 0.2484,  0.0331, -0.0905, -0.4210],\n",
      "        [-0.3981, -0.2984, -0.1482,  0.3410],\n",
      "        [ 0.1848, -0.0753,  0.2463, -0.0203],\n",
      "        [ 0.2847, -0.1195,  0.2070, -0.3939],\n",
      "        [-0.3365,  0.6301,  0.7099, -0.1897],\n",
      "        [-0.1085, -0.5075, -0.4152,  0.1461],\n",
      "        [ 0.0834,  0.0275,  0.4549, -0.1317],\n",
      "        [-0.1200, -0.3987,  0.4774, -0.0362],\n",
      "        [ 0.3368,  0.0294,  0.3368,  0.3747],\n",
      "        [ 0.4258,  0.0456,  0.3290, -0.2051],\n",
      "        [ 0.1978,  0.2010, -0.0378, -0.1838],\n",
      "        [-0.1451, -0.1626, -0.3179,  0.0917]])\n",
      "\n",
      "b_ff2 (Feed forward layer 2 bias) shape: torch.Size([4])\n",
      "tensor([ 0.0695, -0.2181, -0.0060, -0.0344])\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 16  # Dimension of the hidden layer in the feed-forward network which is typically 4 times d_model\n",
    "\n",
    "W_ff1 = torch.randn(d_model, hidden_dim) * 0.3    # Shape (d, hidden_dim) to expand from nxd_model to nxhidden_dim\n",
    "b_ff1 = torch.randn(hidden_dim) * 0.1 # Shape (hidden_dim,) bias for the first feed forward layer where it is broadcasted across the n tokens and added dimension wise\n",
    "W_ff2 = torch.randn(hidden_dim, d_model) * 0.3    # Shape (hidden_dim, d) to project back from nxhidden_dim to nxd_model\n",
    "b_ff2 = torch.randn(d_model) * 0.1 # Shape (d,) bias for the second feed forward layer where it is broadcasted across the n tokens and added dimension wise\n",
    "dropout_ff = nn.Dropout(p=0.2)\n",
    "\n",
    "print(f\"\\nW_ff1 (Feed forward layer 1 weights) shape: {W_ff1.shape}\")\n",
    "print(W_ff1)\n",
    "print(f\"\\nb_ff1 (Feed forward layer 1 bias) shape: {b_ff1.shape}\")\n",
    "print(b_ff1)\n",
    "print(f\"\\nW_ff2 (Feed forward layer 2 weights) shape: {W_ff2.shape}\")\n",
    "print(W_ff2)\n",
    "print(f\"\\nb_ff2 (Feed forward layer 2 bias) shape: {b_ff2.shape}\")\n",
    "print(b_ff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4df94296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForwardNetwork(layernorm_output, W_ff1, b_ff1, W_ff2, b_ff2):\n",
    "    ffn_layer1 = torch.matmul(layernorm_output, W_ff1) + b_ff1 # Shape (n, hidden_dim)\n",
    "    print(\"\\nFeed forward layer 1 output shape before ReLU:\", ffn_layer1.shape)\n",
    "    print(ffn_layer1)\n",
    "\n",
    "    ffn_layer1_activated = F.relu(ffn_layer1) # Shape (n, hidden_dim). ReLU is applied element wise to the output of the first feed forward layer\n",
    "    print(\"\\nFeed forward layer 1 output shape after ReLU:\", ffn_layer1_activated.shape)\n",
    "    ffn_layer1_activated = dropout_ff(ffn_layer1_activated)  # Apply dropout to the activated output of the first feed forward layer\n",
    "    print(ffn_layer1_activated)\n",
    "\n",
    "    ffn_layer2 = torch.matmul(ffn_layer1_activated, W_ff2) + b_ff2 # Shape (n, d)\n",
    "    ffn_layer2 = dropout_ff(ffn_layer2)  # Apply dropout to the output of the second feed forward layer\n",
    "    print(\"\\nFeed forward layer 2 output shape:\", ffn_layer2.shape)\n",
    "    print(ffn_layer2)\n",
    "\n",
    "    return ffn_layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f98d206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feed forward layer 1 output shape before ReLU: torch.Size([9, 16])\n",
      "tensor([[ 5.9478e-02, -7.4387e-03,  6.2545e-02,  5.4143e-01, -7.7687e-01,\n",
      "         -7.0360e-01,  5.7442e-01, -2.7551e-01,  4.4899e-01,  4.6127e-01,\n",
      "         -1.1073e+00, -1.8807e-02, -1.0533e-01,  1.0902e+00, -8.2028e-01,\n",
      "         -1.5090e+00],\n",
      "        [ 1.2885e+00, -2.0279e-01,  9.6221e-02, -3.9086e-02, -1.1629e-01,\n",
      "          5.7048e-01, -4.1414e-01, -4.5242e-02, -3.1493e-01,  1.9056e-01,\n",
      "         -9.3831e-01,  8.2218e-03,  2.4594e-02,  1.1769e-02,  7.1889e-01,\n",
      "          2.9163e-01],\n",
      "        [ 7.0874e-01,  5.4292e-01,  4.2531e-01, -1.8124e-01,  8.7604e-02,\n",
      "         -2.4206e-01, -4.2212e-01, -1.4832e-01, -8.0352e-01, -4.4613e-01,\n",
      "         -1.5677e-01,  1.0491e-01, -2.9955e-01,  5.4469e-01, -2.9363e-01,\n",
      "         -1.6297e+00],\n",
      "        [ 9.9027e-01, -1.9699e-01,  7.7588e-02,  2.9747e-01, -6.3347e-01,\n",
      "          1.6834e-02,  9.2060e-02, -1.6257e-01,  1.4018e-01,  4.8902e-01,\n",
      "         -1.4137e+00, -2.7710e-02, -2.4247e-02,  6.2043e-01,  3.2342e-02,\n",
      "         -5.1592e-01],\n",
      "        [ 3.7430e-01,  2.7917e-01,  2.0790e-01, -4.2455e-01,  8.1278e-01,\n",
      "          4.3301e-01, -7.6459e-01,  4.0689e-03, -9.5266e-01, -6.5794e-01,\n",
      "          7.3224e-01,  1.2434e-01, -8.6129e-02, -4.1559e-01,  6.0701e-01,\n",
      "         -3.6008e-02],\n",
      "        [-3.5014e-01,  1.0036e-01,  6.2966e-02,  5.4749e-01, -6.5644e-01,\n",
      "         -8.9777e-01,  6.4929e-01, -2.9403e-01,  4.4386e-01,  3.2720e-01,\n",
      "         -7.4788e-01,  5.5498e-04, -1.3356e-01,  1.1211e+00, -1.0382e+00,\n",
      "         -1.7635e+00],\n",
      "        [-7.5931e-01, -5.2310e-01, -3.4788e-01,  6.4602e-01, -3.9864e-01,\n",
      "         -2.8663e-01,  7.3432e-01, -1.8407e-01,  9.1311e-01,  6.4265e-01,\n",
      "         -5.1590e-01, -5.3158e-02,  2.0732e-01,  3.5685e-01, -2.0539e-01,\n",
      "          1.4856e-01],\n",
      "        [ 7.6226e-01,  2.5215e-01,  2.5215e-01, -4.0413e-01,  6.1630e-01,\n",
      "          4.7988e-01, -7.8679e-01, -4.4597e-03, -9.5106e-01, -5.3830e-01,\n",
      "          3.2489e-01,  1.0879e-01, -1.0446e-01, -2.8979e-01,  6.3575e-01,\n",
      "         -1.0988e-01],\n",
      "        [-1.3843e+00, -1.4488e-01, -2.6696e-01,  4.0462e-02,  8.7066e-01,\n",
      "          7.7742e-02,  2.5360e-02, -3.7636e-02,  1.7355e-02, -3.3286e-01,\n",
      "          1.3881e+00,  7.5577e-02,  1.7461e-01, -5.7414e-01,  3.0571e-01,\n",
      "          6.5133e-01]])\n",
      "\n",
      "Feed forward layer 1 output shape after ReLU: torch.Size([9, 16])\n",
      "tensor([[7.4347e-02, 0.0000e+00, 7.8182e-02, 6.7679e-01, 0.0000e+00, 0.0000e+00,\n",
      "         7.1802e-01, 0.0000e+00, 5.6123e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.3627e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.6106e+00, 0.0000e+00, 1.2028e-01, 0.0000e+00, 0.0000e+00, 7.1310e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.0742e-02, 1.4711e-02, 8.9861e-01, 3.6453e-01],\n",
      "        [8.8592e-01, 6.7865e-01, 0.0000e+00, 0.0000e+00, 1.0950e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3114e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.2378e+00, 0.0000e+00, 9.6985e-02, 3.7184e-01, 0.0000e+00, 2.1042e-02,\n",
      "         1.1507e-01, 0.0000e+00, 1.7522e-01, 6.1128e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 7.7554e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [4.6787e-01, 3.4897e-01, 2.5987e-01, 0.0000e+00, 1.0160e+00, 5.4127e-01,\n",
      "         0.0000e+00, 5.0861e-03, 0.0000e+00, 0.0000e+00, 9.1530e-01, 1.5543e-01,\n",
      "         0.0000e+00, 0.0000e+00, 7.5877e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 1.2545e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.1161e-01, 0.0000e+00, 5.5482e-01, 4.0900e-01, 0.0000e+00, 6.9372e-04,\n",
      "         0.0000e+00, 1.4014e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 8.0753e-01, 0.0000e+00, 0.0000e+00,\n",
      "         9.1790e-01, 0.0000e+00, 1.1414e+00, 8.0331e-01, 0.0000e+00, 0.0000e+00,\n",
      "         2.5915e-01, 4.4607e-01, 0.0000e+00, 1.8570e-01],\n",
      "        [9.5283e-01, 3.1519e-01, 3.1519e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0611e-01, 1.3598e-01,\n",
      "         0.0000e+00, 0.0000e+00, 7.9468e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0883e+00, 0.0000e+00,\n",
      "         3.1699e-02, 0.0000e+00, 2.1694e-02, 0.0000e+00, 1.7351e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 8.1416e-01]])\n",
      "\n",
      "Feed forward layer 2 output shape: torch.Size([9, 4])\n",
      "tensor([[ 0.0000,  0.0000,  1.0860, -0.7086],\n",
      "        [ 0.2274, -0.0000, -0.0000, -0.2017],\n",
      "        [ 0.6105, -0.1454, -0.6681, -0.0000],\n",
      "        [ 0.7999, -0.3890, -1.1293, -0.0000],\n",
      "        [ 0.7375, -0.1165,  0.0104, -0.8292],\n",
      "        [ 0.7969, -0.0815,  1.1314, -0.5421],\n",
      "        [ 0.0000, -0.1964,  0.9435, -0.3861],\n",
      "        [ 0.0000,  0.1301, -0.5717, -0.0000],\n",
      "        [ 0.4562, -0.3193,  0.5616, -0.8140]])\n"
     ]
    }
   ],
   "source": [
    "ffn_output = feedForwardNetwork(layernorm_output, W_ff1, b_ff1, W_ff2, b_ff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31f4cbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final output shape: torch.Size([9, 4])\n",
      "tensor([[-0.1643, -1.4539,  0.2848,  1.3334],\n",
      "        [-0.6035,  1.5409, -1.1098,  0.1724],\n",
      "        [-1.5909,  0.2955,  0.1268,  1.1686],\n",
      "        [ 0.3898,  0.2581, -1.6603,  1.0124],\n",
      "        [-0.3772,  0.8226,  1.0186, -1.4640],\n",
      "        [ 0.5771, -1.7277,  0.4755,  0.6751],\n",
      "        [ 1.5252, -1.2720, -0.2353, -0.0179],\n",
      "        [-1.2980,  1.4666,  0.1897, -0.3582],\n",
      "        [ 0.8719, -0.5328,  1.0343, -1.3734]])\n"
     ]
    }
   ],
   "source": [
    "final_output = residualPlusLayerNorm(ffn_output, layernorm_output, gamma2, beta2)\n",
    "\n",
    "print(\"\\nFinal output shape:\", final_output.shape)\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5f7b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This completes one block of the transformer architecture (decoder). In practice, multiple such blocks are stacked to form a deep transformer model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
