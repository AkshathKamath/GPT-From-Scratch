{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1307d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d05e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implemented using the Attention notebooks in the Attention directory\n",
    "\n",
    "def selfAttention(input_embeddings, W_q, W_k, W_v, W_o):\n",
    "    n = input_embeddings.shape[0]\n",
    "    d_model = input_embeddings.shape[1]\n",
    "    d_k = W_q.shape[1]\n",
    "\n",
    "    Q = torch.matmul(input_embeddings, W_q)\n",
    "    K = torch.matmul(input_embeddings, W_k)\n",
    "    V = torch.matmul(input_embeddings, W_v)\n",
    "\n",
    "    mask  = torch.tril(torch.ones(n, n))\n",
    "\n",
    "    attention_scores = torch.matmul(Q, K.T)\n",
    "    masked_attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "    masked_attention_scores /= torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "    attention_weights = F.softmax(masked_attention_scores, dim=-1)\n",
    "    dropout = nn.Dropout(p=0.2)\n",
    "    attention_weights = dropout(attention_weights)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    final_output = torch.matmul(output, W_o)\n",
    "    final_output = dropout(final_output)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30527c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implemented using the LayerNorm notebook in the LayerNorm directory\n",
    "def residualPlusLayerNorm(attention_output, input_embeddings, gamma, beta, eps = 1e-5,):\n",
    "    residual_output = attention_output + input_embeddings\n",
    "\n",
    "    means = torch.mean(residual_output, dim=-1, keepdim=True) # Shape (n, 1)\n",
    "    variances = torch.var(residual_output, dim=-1, keepdim=True, unbiased=False) # Shape (n, 1)\n",
    "    normalized = (residual_output - means) / torch.sqrt(variances + eps) # Shape (n, d)\n",
    "\n",
    "    ln_output = normalized * gamma + beta\n",
    "    return ln_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee10e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implemented using the FeedForwardNetwork notebook in the Transformer directory\n",
    "def feedForwardNetwork(layernorm_output, W_ff1, b_ff1, W_ff2, b_ff2):\n",
    "    ffn_layer1 = torch.matmul(layernorm_output, W_ff1) + b_ff1\n",
    "    ffn_layer1_activated = F.gelu(ffn_layer1)\n",
    "    ffn_layer1_activated = nn.Dropout(p=0.2)(ffn_layer1_activated)\n",
    "\n",
    "    ffn_layer2 = torch.matmul(ffn_layer1_activated, W_ff2) + b_ff2\n",
    "    ffn_layer2 = nn.Dropout(p=0.2)(ffn_layer2)\n",
    "    return ffn_layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a87c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "Number of tokens: 9\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "## Simple tokenization by splitting on spaces, ideally more complex tokenization would be used like BPE or WordPiece\n",
    "sentence = sentence.split()\n",
    "n = len(sentence)\n",
    "\n",
    "print(f\"Tokenized sentence: {sentence}\")\n",
    "print(f\"Number of tokens: {len(sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4eb2aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 23\n",
      "Vocabulary: ['<PAD>', '<START>', '<END>', '<UNK>', 'The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'cat', 'runs', 'fast', 'slowly', 'and', 'a', 'is', 'are', 'was', 'were']\n"
     ]
    }
   ],
   "source": [
    "## Sample vocabulary, ideally this would be a much larger vocabulary\n",
    "vocab = [\"<PAD>\", \"<START>\", \"<END>\", \"<UNK>\"] + sentence + [\"cat\", \"runs\", \"fast\", \"slowly\", \"and\", \"the\", \"a\", \"is\", \"are\", \"was\", \"were\"]\n",
    "\n",
    "vocab = list(dict.fromkeys(vocab)) # Remove duplicates\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Vocabulary: {vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "702f9e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word to index mapping: {'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNK>': 3, 'The': 4, 'quick': 5, 'brown': 6, 'fox': 7, 'jumps': 8, 'over': 9, 'the': 10, 'lazy': 11, 'dog': 12, 'cat': 13, 'runs': 14, 'fast': 15, 'slowly': 16, 'and': 17, 'a': 18, 'is': 19, 'are': 20, 'was': 21, 'were': 22}\n",
      "Index to word mapping: {0: '<PAD>', 1: '<START>', 2: '<END>', 3: '<UNK>', 4: 'The', 5: 'quick', 6: 'brown', 7: 'fox', 8: 'jumps', 9: 'over', 10: 'the', 11: 'lazy', 12: 'dog', 13: 'cat', 14: 'runs', 15: 'fast', 16: 'slowly', 17: 'and', 18: 'a', 19: 'is', 20: 'are', 21: 'was', 22: 'were'}\n"
     ]
    }
   ],
   "source": [
    "## Creatinga a word to index and index to word mapping to represent text as integers\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "print(f\"Word to index mapping: {word_to_idx}\")\n",
    "print(f\"Index to word mapping: {idx_to_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97002c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input indices: [4, 5, 6, 7, 8, 9, 10, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "## Convert the sentence to a list of indices\n",
    "input_indices = [word_to_idx[word] for word in sentence]\n",
    "\n",
    "print(f\"Input indices: {input_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "715d1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 8      # embedding dimension\n",
    "d_k = 6          # attention dimension\n",
    "hidden_dim = 16  # feed forward network hidden dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d750a754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: torch.Size([23, 8])\n",
      "Embedding matrix: tensor([[ 5.7807e-01,  4.4619e-01,  2.7022e-01, -6.3166e-01,  2.0353e-01,\n",
      "         -3.7036e-01, -1.2920e-02, -4.8140e-01],\n",
      "        [-2.2564e-01,  4.9462e-01, -1.1774e-01, -4.2108e-01, -2.1836e-01,\n",
      "         -1.6783e-01, -2.3065e-01,  2.2873e-01],\n",
      "        [ 4.9270e-01, -4.7879e-02, -1.4922e-01,  1.3188e-01, -2.2744e-01,\n",
      "          3.2350e-01,  2.4024e-01,  5.0419e-01],\n",
      "        [ 3.8374e-01,  3.8893e-01,  1.8314e-01,  4.0042e-01, -6.9487e-02,\n",
      "          1.2528e-02, -7.5473e-02,  2.5796e-01],\n",
      "        [-4.1540e-01, -2.6137e-01, -6.7010e-02,  5.1521e-01,  9.5664e-02,\n",
      "         -1.2736e-01,  9.1716e-02, -2.3238e-01],\n",
      "        [-4.6727e-01,  2.9869e-01, -2.6394e-01, -1.8034e-01, -3.8225e-01,\n",
      "          6.3684e-01, -3.7040e-01, -1.4637e-01],\n",
      "        [-2.7415e-01, -1.9744e-01,  2.3407e-02,  1.5774e-01, -1.4640e-01,\n",
      "          3.5741e-01, -2.4420e-01, -2.2080e-01],\n",
      "        [-4.2097e-01,  1.0801e-02, -1.9043e-02,  2.0268e-01, -2.9342e-02,\n",
      "          5.5338e-01, -3.5536e-01,  4.1506e-01],\n",
      "        [ 4.3354e-01,  2.5692e-01,  6.6542e-01,  1.5695e-01,  1.0399e-01,\n",
      "         -5.9199e-02, -3.1638e-01,  3.8340e-01],\n",
      "        [-5.1657e-02,  1.5714e-01,  1.6987e-02,  1.2789e-01,  1.7250e-01,\n",
      "         -1.9252e-01, -6.6192e-01, -2.2524e-01],\n",
      "        [ 3.2604e-03, -1.0162e-01, -4.0220e-01, -1.7561e-01,  1.6086e-01,\n",
      "          1.5739e-01,  3.4236e-01,  1.5493e-02],\n",
      "        [ 2.2319e-01, -1.4448e-01, -3.1484e-01,  1.8117e-01, -5.1669e-01,\n",
      "         -2.4833e-01,  4.0041e-01,  1.4506e-01],\n",
      "        [-7.5286e-01,  1.4640e-01,  2.3538e-01,  8.5942e-03,  1.9223e-01,\n",
      "          1.7497e-01,  3.2008e-01, -1.3505e-01],\n",
      "        [-5.5580e-02,  2.2583e-01,  1.2143e-01,  5.3540e-02,  7.9473e-02,\n",
      "          3.8195e-01, -3.9326e-04, -9.1081e-02],\n",
      "        [-4.3711e-01, -3.0701e-02, -1.7975e-01,  1.4312e-01,  2.1785e-01,\n",
      "          2.7346e-02, -1.1672e-01,  1.5837e-01],\n",
      "        [-3.8056e-03,  7.2251e-02,  3.9761e-02,  2.2927e-01,  3.2850e-01,\n",
      "          1.0197e-01,  2.1599e-01,  1.2342e-01],\n",
      "        [ 5.7935e-01,  3.0356e-01, -4.3092e-01, -3.3896e-01, -4.0810e-02,\n",
      "          4.9062e-01,  1.9642e-01,  1.7280e-01],\n",
      "        [ 3.4245e-01,  5.5694e-03, -5.4174e-01,  2.7763e-01, -1.1260e-01,\n",
      "          3.0993e-01, -2.0600e-01,  1.9104e-01],\n",
      "        [-2.9180e-01,  2.8754e-01,  4.8576e-01,  4.3518e-01,  8.0844e-02,\n",
      "         -6.3113e-02, -2.1984e-01,  3.1289e-02],\n",
      "        [ 1.0463e-01,  2.9028e-01, -1.3971e-01,  4.8144e-01, -7.4404e-01,\n",
      "         -1.2526e-01, -3.5864e-01,  2.4370e-01],\n",
      "        [-5.7017e-01,  6.8573e-02,  7.4578e-03, -1.0379e-01,  8.6050e-02,\n",
      "         -2.1925e-01,  5.2446e-02, -3.2818e-01],\n",
      "        [ 2.1496e-01,  4.6004e-01, -4.3529e-01, -2.3584e-01, -2.8689e-01,\n",
      "         -3.7428e-01, -2.2498e-01, -1.7766e-01],\n",
      "        [-4.5979e-01, -2.1754e-01,  1.3992e-01,  2.0002e-01, -1.3161e-02,\n",
      "          7.1042e-02, -2.1182e-01, -2.1507e-01]])\n"
     ]
    }
   ],
   "source": [
    "## Sample embedding matrix, ideally this would be learned during training or loaded from a pre-trained model\n",
    "torch.manual_seed(42)\n",
    "embedding_matrix = torch.randn(vocab_size, d_model) * 0.3 # Shape (vocab_size, d_model) i.e. for every token in the vocab, we have a d_model dimensional embedding\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "print(f\"Embedding matrix: {embedding_matrix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a7cfb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings shape: torch.Size([9, 8])\n",
      "Input embeddings: tensor([[-0.4154, -0.2614, -0.0670,  0.5152,  0.0957, -0.1274,  0.0917, -0.2324],\n",
      "        [-0.4673,  0.2987, -0.2639, -0.1803, -0.3822,  0.6368, -0.3704, -0.1464],\n",
      "        [-0.2741, -0.1974,  0.0234,  0.1577, -0.1464,  0.3574, -0.2442, -0.2208],\n",
      "        [-0.4210,  0.0108, -0.0190,  0.2027, -0.0293,  0.5534, -0.3554,  0.4151],\n",
      "        [ 0.4335,  0.2569,  0.6654,  0.1569,  0.1040, -0.0592, -0.3164,  0.3834],\n",
      "        [-0.0517,  0.1571,  0.0170,  0.1279,  0.1725, -0.1925, -0.6619, -0.2252],\n",
      "        [ 0.0033, -0.1016, -0.4022, -0.1756,  0.1609,  0.1574,  0.3424,  0.0155],\n",
      "        [ 0.2232, -0.1445, -0.3148,  0.1812, -0.5167, -0.2483,  0.4004,  0.1451],\n",
      "        [-0.7529,  0.1464,  0.2354,  0.0086,  0.1922,  0.1750,  0.3201, -0.1350]])\n"
     ]
    }
   ],
   "source": [
    "## Using the embedding matrix to convert input indices to embeddings\n",
    "input_embeddings = embedding_matrix[input_indices]\n",
    "\n",
    "print(f\"Input embeddings shape: {input_embeddings.shape}\")\n",
    "print(f\"Input embeddings: {input_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de87a459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings with positional encoding shape: torch.Size([9, 8])\n",
      "Input embeddings with positional encoding: tensor([[-0.5591, -0.4329,  0.1523,  0.5978,  0.3848, -0.1071, -0.0298, -0.2091],\n",
      "        [-0.5186,  0.1834, -0.8415, -0.6078, -0.3766,  0.9540, -0.3849, -0.1913],\n",
      "        [-0.0169, -0.0000,  0.3128,  0.1083, -0.0000,  0.9466, -0.0000, -0.0000],\n",
      "        [-0.5209, -0.1938, -0.2940,  0.0000,  0.0901,  0.0000, -0.3332,  0.3378],\n",
      "        [ 0.4266,  0.3052,  0.4901,  0.2786, -0.1157,  0.0014, -0.3508,  0.4469],\n",
      "        [-0.4584,  0.7591,  0.2715,  0.5009,  0.3740, -0.1394, -0.7420, -0.3369],\n",
      "        [ 0.0473,  0.1358, -0.5009, -0.2389,  0.3617,  0.3403,  0.5746,  0.0147],\n",
      "        [-0.0000, -0.0241, -1.0398,  0.2205, -0.0000, -0.0000,  0.0000,  0.0000],\n",
      "        [-0.6428,  0.6673,  0.0000,  0.2560,  0.3439,  0.5079,  0.4674, -0.0000]])\n"
     ]
    }
   ],
   "source": [
    "## Sample positional embeddings, ideally these would be learned during training\n",
    "positional_embeddings = torch.randn(n, d_model) * 0.2\n",
    "input_embeddings = input_embeddings + positional_embeddings\n",
    "dropout = nn.Dropout(p=0.2)\n",
    "input_embeddings = dropout(input_embeddings)\n",
    "\n",
    "\n",
    "print(f\"Input embeddings with positional encoding shape: {input_embeddings.shape}\")\n",
    "print(f\"Input embeddings with positional encoding: {input_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ca41ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Self Attention weights for the K, Q and V matrices\n",
    "W_q = torch.randn(d_model, d_k) * 0.3 # Shape (d_model, d_k)\n",
    "W_k = torch.randn(d_model, d_k) * 0.3 # Shape (d_model, d_k)\n",
    "W_v = torch.randn(d_model, d_k) * 0.3 # Shape (d_model, d_k)\n",
    "W_o = torch.randn(d_k, d_model) * 0.3 # Shape (d_k, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9460955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Layer Norm weights for both before and after the feed forward network\n",
    "gamma1 = torch.ones(d_model) # Shape (d_model,)\n",
    "beta1 = torch.zeros(d_model) # Shape (d_model,)\n",
    "gamma2 = torch.ones(d_model) # Shape (d_model,)\n",
    "beta2 = torch.zeros(d_model) # Shape (d_model,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c929194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feed Forward Network weights\n",
    "W_ff1 = torch.randn(d_model, hidden_dim) * 0.3 # Shape (d_model, hidden_dim)\n",
    "b_ff1 = torch.randn(hidden_dim) * 0.3 # Shape (hidden_dim,)\n",
    "W_ff2 = torch.randn(hidden_dim, d_model) * 0.3 # Shape (hidden_dim, d_model)\n",
    "b_ff2 = torch.randn(d_model) * 0.3 # Shape (d_model,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d78ba5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output shape: torch.Size([9, 8])\n",
      "Attention output: tensor([[ 0.1218, -0.2799,  0.8330, -0.0000,  0.2833,  0.6572, -0.4778,  0.0000],\n",
      "        [-0.0373,  0.3466,  0.3091,  0.2591,  0.5623,  0.0320,  0.4537, -0.1644],\n",
      "        [ 0.0000,  0.1883,  0.3220,  0.2420,  0.0000,  0.0789,  0.6034, -0.0000],\n",
      "        [ 0.0086,  0.1428,  0.3652,  0.1505,  0.3699,  0.1306,  0.0000, -0.1026],\n",
      "        [ 0.0225,  0.0891,  0.0000,  0.1145,  0.2571,  0.1018,  0.2894, -0.1312],\n",
      "        [ 0.0887, -0.0000,  0.0000,  0.0152,  0.0965,  0.0000,  0.0629, -0.1278],\n",
      "        [ 0.0000,  0.0571,  0.0000,  0.0839,  0.1720,  0.0666,  0.2225, -0.1060],\n",
      "        [-0.0000,  0.1464,  0.1313,  0.1089,  0.0000,  0.0000,  0.2304, -0.0000],\n",
      "        [ 0.0405,  0.0428,  0.1954, -0.0017,  0.1252,  0.1464, -0.0161, -0.0000]])\n"
     ]
    }
   ],
   "source": [
    "attention_output = selfAttention(input_embeddings, W_q, W_k, W_v, W_o)\n",
    "\n",
    "print(f\"Attention output shape: {attention_output.shape}\")\n",
    "print(f\"Attention output: {attention_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb66bddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm output shape: torch.Size([9, 8])\n",
      "LayerNorm output: tensor([[-0.9096, -1.3619,  1.4257,  0.7895,  0.9050,  0.7112, -1.0250, -0.5350],\n",
      "        [-1.0714,  1.0317, -1.0256, -0.6699,  0.3650,  1.9149,  0.1387, -0.6835],\n",
      "        [-1.0288, -0.4504,  0.8077,  0.0060, -0.9812,  1.9087,  0.7191, -0.9812],\n",
      "        [-1.8151, -0.2389,  0.1787,  0.4498,  1.5076,  0.3819, -1.2032,  0.7392],\n",
      "        [ 0.9360,  0.6353,  1.1599,  0.6293, -0.7486, -0.9580, -1.8591,  0.2053],\n",
      "        [-0.8402,  1.4437,  0.4572,  0.9520,  0.8597, -0.3742, -1.4660, -1.0323],\n",
      "        [-0.2729,  0.1001, -1.6771, -0.7911,  0.9731,  0.6481,  1.6478, -0.6280],\n",
      "        [ 0.0802,  0.4268, -2.4951,  1.0139,  0.0802,  0.0802,  0.7334,  0.0802],\n",
      "        [-2.1949,  1.1207, -0.1796, -0.0309,  0.5118,  0.9795,  0.4667, -0.6733]])\n"
     ]
    }
   ],
   "source": [
    "layernorm_output1 = residualPlusLayerNorm(attention_output, input_embeddings, gamma1, beta1)\n",
    "\n",
    "print(f\"LayerNorm output shape: {layernorm_output1.shape}\")\n",
    "print(f\"LayerNorm output: {layernorm_output1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8a25920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed Forward Network output shape: torch.Size([9, 8])\n",
      "Feed Forward Network output: tensor([[ 0.0000, -1.4984,  0.1647,  0.0000, -2.2981, -0.9139, -0.5148, -0.2643],\n",
      "        [ 0.0495, -0.0000, -1.8511,  1.5845,  1.0706,  0.7957, -0.8447, -1.8805],\n",
      "        [ 0.0000, -0.3541, -0.6420, -0.3679, -0.3375, -0.0000, -1.6751, -0.5623],\n",
      "        [ 1.2290, -1.4242, -1.2835,  0.7632, -1.7395,  0.8066, -0.0000, -0.0000],\n",
      "        [-0.3013, -1.5876, -0.0000,  0.0955, -0.2977,  0.7533, -0.0859, -0.3359],\n",
      "        [ 2.1650, -1.7628, -0.4042,  0.0000, -2.2275,  1.1990, -0.6661, -0.6499],\n",
      "        [ 1.2242, -1.2095, -2.8590,  4.0536,  0.0400,  4.1343, -1.6785, -2.2926],\n",
      "        [ 0.0000, -0.0000, -2.2299,  0.5013,  0.0000,  1.2136, -0.6072, -0.0000],\n",
      "        [ 0.8520, -0.2528, -2.0101,  2.3380, -0.6107,  2.5460, -1.2218, -1.3421]])\n"
     ]
    }
   ],
   "source": [
    "feedforward_output = feedForwardNetwork(layernorm_output1, W_ff1, b_ff1, W_ff2, b_ff2)\n",
    "\n",
    "print(f\"Feed Forward Network output shape: {feedforward_output.shape}\")\n",
    "print(f\"Feed Forward Network output: {feedforward_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e61e195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final LayerNorm output shape: torch.Size([9, 8])\n",
      "Final LayerNorm output: tensor([[-0.1870, -1.6827,  1.7297,  1.1157, -0.5578,  0.3549, -0.6702, -0.1025],\n",
      "        [-0.4781,  0.6284, -1.4775,  0.5653,  0.8460,  1.5330, -0.3079, -1.3090],\n",
      "        [-0.5173, -0.3010,  0.6345,  0.1258, -0.7968,  2.3154, -0.4471, -1.0136],\n",
      "        [-0.3599, -1.3797, -0.8510,  1.3437, -0.0245,  1.3205, -0.9442,  0.8951],\n",
      "        [ 0.8694, -0.7450,  1.4038,  0.9610, -0.8406,  0.0155, -1.7549,  0.0909],\n",
      "        [ 1.3209, -0.0211,  0.2827,  1.0166, -0.8771,  0.9128, -1.5011, -1.1338],\n",
      "        [ 0.2707, -0.4493, -1.6466,  1.0782,  0.2923,  1.6092, -0.0724, -1.0821],\n",
      "        [ 0.1215,  0.3124, -2.5259,  0.9121,  0.1215,  0.7901,  0.1468,  0.1215],\n",
      "        [-0.7159,  0.4308, -1.1551,  1.1773, -0.0706,  1.8093, -0.4111, -1.0648]])\n"
     ]
    }
   ],
   "source": [
    "layernorm_output2 = residualPlusLayerNorm(feedforward_output, layernorm_output1, gamma2, beta2)\n",
    "\n",
    "print(f\"Final LayerNorm output shape: {layernorm_output2.shape}\")\n",
    "print(f\"Final LayerNorm output: {layernorm_output2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77889b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM head weights shape: torch.Size([8, 23])\n",
      "LM head bias shape: torch.Size([23])\n"
     ]
    }
   ],
   "source": [
    "## Weights for the final linear layer to project to vocab size i.e. the language model head\n",
    "W_lm_head = torch.randn(d_model, vocab_size) * 0.3 # Shape (d_model, vocab_size)\n",
    "b_lm_head = torch.randn(vocab_size) * 0.1 # Shape (vocab_size,)\n",
    "\n",
    "print(f\"LM head weights shape: {W_lm_head.shape}\")\n",
    "print(f\"LM head bias shape: {b_lm_head.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3144ec47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([9, 23])\n"
     ]
    }
   ],
   "source": [
    "## Passing the final output through the LM head to get logits for each token in the vocabulary\n",
    "logits = torch.matmul(layernorm_output2, W_lm_head) + b_lm_head # Shape (n, vocab_size)\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e372d6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities shape: torch.Size([9, 23])\n",
      "Probabilities: tensor([[0.0557, 0.0177, 0.1232, 0.0408, 0.0224, 0.2360, 0.0191, 0.0245, 0.0175,\n",
      "         0.0056, 0.0596, 0.0256, 0.0242, 0.0069, 0.0076, 0.0331, 0.1046, 0.0062,\n",
      "         0.0231, 0.0837, 0.0228, 0.0242, 0.0159],\n",
      "        [0.0167, 0.0356, 0.0039, 0.0197, 0.0758, 0.0092, 0.0298, 0.0162, 0.0780,\n",
      "         0.0367, 0.0536, 0.0316, 0.0746, 0.0271, 0.0418, 0.1728, 0.0055, 0.0463,\n",
      "         0.0384, 0.1243, 0.0313, 0.0226, 0.0086],\n",
      "        [0.0111, 0.0597, 0.0132, 0.0144, 0.0674, 0.0825, 0.0382, 0.0328, 0.0195,\n",
      "         0.0055, 0.0270, 0.0276, 0.1094, 0.0096, 0.0116, 0.1526, 0.0677, 0.0101,\n",
      "         0.0648, 0.0806, 0.0609, 0.0216, 0.0121],\n",
      "        [0.0669, 0.0753, 0.0348, 0.0370, 0.0295, 0.0854, 0.0111, 0.0192, 0.0655,\n",
      "         0.0220, 0.0802, 0.0357, 0.0408, 0.0232, 0.1192, 0.0492, 0.0503, 0.0253,\n",
      "         0.0085, 0.0701, 0.0252, 0.0140, 0.0117],\n",
      "        [0.0385, 0.0157, 0.0631, 0.0350, 0.0303, 0.2713, 0.0361, 0.0351, 0.0211,\n",
      "         0.0123, 0.0506, 0.0476, 0.0197, 0.0081, 0.0242, 0.0200, 0.1059, 0.0146,\n",
      "         0.0112, 0.0671, 0.0321, 0.0194, 0.0209],\n",
      "        [0.0184, 0.0251, 0.0135, 0.0293, 0.1042, 0.1397, 0.0561, 0.0224, 0.0243,\n",
      "         0.0114, 0.0672, 0.0492, 0.0414, 0.0079, 0.0350, 0.0582, 0.0492, 0.0334,\n",
      "         0.0160, 0.1279, 0.0439, 0.0162, 0.0100],\n",
      "        [0.0282, 0.0577, 0.0063, 0.0308, 0.0973, 0.0213, 0.0218, 0.0096, 0.0490,\n",
      "         0.0190, 0.0892, 0.0255, 0.0506, 0.0193, 0.0606, 0.1419, 0.0111, 0.0457,\n",
      "         0.0179, 0.1485, 0.0286, 0.0151, 0.0051],\n",
      "        [0.0297, 0.0793, 0.0066, 0.0384, 0.0561, 0.0173, 0.0136, 0.0106, 0.0697,\n",
      "         0.0409, 0.0564, 0.0245, 0.0502, 0.0386, 0.2207, 0.0429, 0.0107, 0.1083,\n",
      "         0.0111, 0.0303, 0.0191, 0.0149, 0.0100],\n",
      "        [0.0145, 0.0494, 0.0068, 0.0246, 0.0838, 0.0342, 0.0198, 0.0149, 0.0666,\n",
      "         0.0143, 0.0639, 0.0309, 0.1561, 0.0155, 0.0416, 0.1267, 0.0155, 0.0441,\n",
      "         0.0414, 0.0835, 0.0249, 0.0190, 0.0081]])\n"
     ]
    }
   ],
   "source": [
    "## Applying softmax to get probabilities for each token in the vocabulary, where each row represents the probabilities for each word in the vocab for the next token given the previous tokens\n",
    "probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "print(f\"Probabilities shape: {probabilities.shape}\")\n",
    "print(f\"Probabilities: {probabilities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7b94f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000]])\n"
     ]
    }
   ],
   "source": [
    "row_sum = probabilities.sum(dim=-1, keepdim=True)  # Validate trhat each row sums to 1 (pdf)\n",
    "print(row_sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
