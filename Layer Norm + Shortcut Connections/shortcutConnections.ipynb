{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ca059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59241a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), nn.ReLU()), ## Each is a layer with weigh matrix of shape (layer_sizes[i], layer_sizes[i+1]) i.e. having layer[i+1] neurons and then a ReLU activation function\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), nn.ReLU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), nn.ReLU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), nn.ReLU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), nn.ReLU()),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output ## For every layer, we add the input of the layer to the output of the layer if the input and output shapes match\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00adf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1] ## 3 x 3 weight matrices for each layer, and the last layer has 1 neuron with a 3 x 1 weight matrix\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123) # specify random seed for the initial weights for reproducibility\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=False # No shortcut connections\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "693b9333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss() ## (y - 0)^2\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights ## The mean of the gradients for each weight in the weight matrix\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35598c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.0013751330552622676\n",
      "layers.1.0.weight has gradient mean of 0.0038167957682162523\n",
      "layers.2.0.weight has gradient mean of 0.0076410952024161816\n",
      "layers.3.0.weight has gradient mean of 0.007722062990069389\n",
      "layers.4.0.weight has gradient mean of 0.04962990805506706\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input) ## We can see that the gradients for the earlier layers are getting much smaller than the gradients for the later layers, which is a sign of vanishing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b9c2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.5557742714881897\n",
      "layers.1.0.weight has gradient mean of 0.09135337918996811\n",
      "layers.2.0.weight has gradient mean of 0.7913904190063477\n",
      "layers.3.0.weight has gradient mean of 0.21711303293704987\n",
      "layers.4.0.weight has gradient mean of 3.140749216079712\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=True ## With shortcut connections\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input) ## Much better gradient flow through the network, as the gradients for all layers are of similar magnitude. Also the loss function graph is much smoother and converges faster, with lesser local minima and saddle points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
