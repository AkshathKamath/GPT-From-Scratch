{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8b8b963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c9194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using for loop to loop through the heads and compute the attention scores is very slow, we need to optimize with tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba46ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Also we use batches of data here to improve performance instead of looping through the dataset individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0 ## d_model must be divisible by num_heads, so that we can break the Q, K and V matrices into num_heads x n x d_k\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        ## Randomly initialize the W_q, W_k and W_v matrices to d x d shape (we will break the Q, K and V to num_heads x d_k). Instead of num_heads x d x d_k, we initialize as d x d \n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        ## Initialize the dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        ## Initialize the W_q, W_k and W_v matrices as a normal distribution with mean 0 and std 0.02\n",
    "        for module in [self.W_q, self.W_k, self.W_v, self.W_o]:\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    \n",
    "    def forward(self, input_embeddings):\n",
    "        ## input_embeddings have shape of n x d where n is number of tokens (window size) and d is embedding dimension\n",
    "        n, d_model = input_embeddings.shape\n",
    "\n",
    "        mask = torch.tril(torch.ones(n, n))\n",
    "\n",
    "        ## Compute the Q, K and V matrices together before splitting them into heads. Instead of running a loop and creating individual Q, K and V matrices of shape n x d_k for each head, we store the Q, K and V matrices of all heads in 3 matrices for each of shape n x d, which will later be split into num_heads x n x d_k\n",
    "        Q_all = self.W_q(input_embeddings) # Shape: n x d\n",
    "        K_all = self.W_k(input_embeddings) # Shape: n x d\n",
    "        V_all = self.W_v(input_embeddings) # Shape: n x d\n",
    "\n",
    "        ## Change from shape n x d to n x num_heads x d_k by spliiting them into separate heads\n",
    "        Q = Q_all.view(n, self.num_heads, self.d_k)\n",
    "        K = K_all.view(n, self.num_heads, self.d_k)\n",
    "        V = V_all.view(n, self.num_heads, self.d_k)\n",
    "\n",
    "        Q = Q.transpose(0, 1) # Shape: num_heads x n x d_k\n",
    "        K = K.transpose(0, 1) # Shape: num_heads x n x d_k\n",
    "        V = V.transpose(0, 1) # Shae: num_heads x n x d_k\n",
    "\n",
    "        # (num_heads, n, d_k) @ (num_heads, d_k, n) -> (num_heads, n, n)\n",
    "        ## So for every head we have gotten the attention scores together in 1 matrix\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        ## Apply masking to mask out scores of future tokens\n",
    "        attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1) # Shape: num_heads x n x n\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # (num_heads, n, n) @ (num_heads, n, d_k) -> (num_heads,n, d_k)\n",
    "        ## For every head, we get the attention outputs in 1 matrix\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        ## Now we want to do the reverse operations, converting from num_heads x n x d_k to n x d\n",
    "        # (num_heads, n, d_k) -> (n, num_heads, d_k)\n",
    "        attention_output = attention_output.transpose(0, 1).contiguous()\n",
    "        # (n, num_heads, d_k) -> (n, d)\n",
    "        attention_output = attention_output.view(n, self.d_model)\n",
    "\n",
    "        ## So head outputs can interact with each other\n",
    "        # n x d @ d x d -> n x d\n",
    "        output = self.W_o(attention_output)\n",
    "\n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb99a17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "Number of tokens: 9\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "## Simple tokenization by splitting on spaces, ideally more complex tokenization would be used like BPE or WordPiece\n",
    "sentence = sentence.split()\n",
    "n = len(sentence)\n",
    "\n",
    "print(f\"Tokenized sentence: {sentence}\")\n",
    "print(f\"Number of tokens: {len(sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "495639b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings (4-dimensional):\n",
      "  The : tensor([1.0000, 0.5000, 0.2000, 0.8000])\n",
      "  quick: tensor([0.3000, 1.0000, 0.7000, 0.1000])\n",
      "  brown: tensor([0.6000, 0.2000, 1.0000, 0.4000])\n",
      "  fox : tensor([0.9000, 0.8000, 0.3000, 1.0000])\n",
      "  jumps: tensor([0.4000, 0.6000, 0.8000, 0.2000])\n",
      "  over: tensor([0.7000, 0.3000, 0.5000, 0.9000])\n",
      "  the : tensor([1.0000, 0.5000, 0.2000, 0.8000])\n",
      "  lazy: tensor([0.2000, 0.9000, 0.4000, 0.6000])\n",
      "  dog : tensor([0.8000, 0.4000, 0.9000, 0.3000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Sample word embeddings, ideally these would be learned in the language modelling process or loaded from a pre-trained model like GloVe or Word2Vec\n",
    "\n",
    "# Shape of embeddings: (n, d) where n is number of tokens and d is embedding dimension\n",
    "embeddings = torch.tensor([\n",
    "        [1.0, 0.5, 0.2, 0.8], \n",
    "        [0.3, 1.0, 0.7, 0.1],  \n",
    "        [0.6, 0.2, 1.0, 0.4],  \n",
    "        [0.9, 0.8, 0.3, 1.0],  \n",
    "        [0.4, 0.6, 0.8, 0.2],  \n",
    "        [0.7, 0.3, 0.5, 0.9],  \n",
    "        [1.0, 0.5, 0.2, 0.8],  \n",
    "        [0.2, 0.9, 0.4, 0.6],  \n",
    "        [0.8, 0.4, 0.9, 0.3]  \n",
    "    ])\n",
    "\n",
    "print(\"Word embeddings (4-dimensional):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  {word:4}: {embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "251ecbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embeddings (Same dimesnions as word embeddings):\n",
      "  Pos 0 (The): tensor([0., 1., 0., 1.])\n",
      "  Pos 1 (quick): tensor([0.1000, 0.9000, 0.1000, 0.9000])\n",
      "  Pos 2 (brown): tensor([0.2000, 0.8000, 0.2000, 0.8000])\n",
      "  Pos 3 (fox): tensor([0.3000, 0.7000, 0.3000, 0.7000])\n",
      "  Pos 4 (jumps): tensor([0.4000, 0.6000, 0.4000, 0.6000])\n",
      "  Pos 5 (over): tensor([0.5000, 0.5000, 0.5000, 0.5000])\n",
      "  Pos 6 (the): tensor([0.6000, 0.4000, 0.6000, 0.4000])\n",
      "  Pos 7 (lazy): tensor([0.7000, 0.3000, 0.7000, 0.3000])\n",
      "  Pos 8 (dog): tensor([0.8000, 0.2000, 0.8000, 0.2000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Sample positional encodings, typically these would be generated using math functions or learned during training or RoPE\n",
    "\n",
    "positional_embeddings = torch.tensor([\n",
    "    [0.0, 1.0, 0.0, 1.0],  \n",
    "    [0.1, 0.9, 0.1, 0.9],  \n",
    "    [0.2, 0.8, 0.2, 0.8],  \n",
    "    [0.3, 0.7, 0.3, 0.7],  \n",
    "    [0.4, 0.6, 0.4, 0.6],  \n",
    "    [0.5, 0.5, 0.5, 0.5],  \n",
    "    [0.6, 0.4, 0.6, 0.4],  \n",
    "    [0.7, 0.3, 0.7, 0.3],  \n",
    "    [0.8, 0.2, 0.8, 0.2]   \n",
    "])\n",
    "\n",
    "print(\"Positional embeddings (Same dimesnions as word embeddings):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  Pos {i} ({word}): {positional_embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b06247e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings (word + positional):\n",
      "  The : tensor([1.0000, 1.5000, 0.2000, 1.8000])\n",
      "  quick: tensor([0.4000, 1.9000, 0.8000, 1.0000])\n",
      "  brown: tensor([0.8000, 1.0000, 1.2000, 1.2000])\n",
      "  fox : tensor([1.2000, 1.5000, 0.6000, 1.7000])\n",
      "  jumps: tensor([0.8000, 1.2000, 1.2000, 0.8000])\n",
      "  over: tensor([1.2000, 0.8000, 1.0000, 1.4000])\n",
      "  the : tensor([1.6000, 0.9000, 0.8000, 1.2000])\n",
      "  lazy: tensor([0.9000, 1.2000, 1.1000, 0.9000])\n",
      "  dog : tensor([1.6000, 0.6000, 1.7000, 0.5000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## The final input to the Attention block is the sum of the word embeddings and positional encodings\n",
    "\n",
    "input_embeddings = embeddings + positional_embeddings\n",
    "\n",
    "print(\"Input embeddings (word + positional):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  {word:4}: {input_embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98b314e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 4      # embedding dimension of the tokens\n",
    "num_heads = 2    # number of attention heads\n",
    "d_k = d_model // num_heads # Dimension of the Q, K and V matrices for each head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82077105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadedAttention(\n",
      "  (W_q): Linear(in_features=4, out_features=4, bias=False)\n",
      "  (W_k): Linear(in_features=4, out_features=4, bias=False)\n",
      "  (W_v): Linear(in_features=4, out_features=4, bias=False)\n",
      "  (W_o): Linear(in_features=4, out_features=4, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "attentionBlock = MultiHeadedAttention(d_model, num_heads, dropout=0.2)\n",
    "print(attentionBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fb04189",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output, attention_weights = attentionBlock(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc334a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 4])\n",
      "tensor([[ 0.0007, -0.0025,  0.0002, -0.0016],\n",
      "        [ 0.0018, -0.0030,  0.0018, -0.0007],\n",
      "        [ 0.0012, -0.0024,  0.0002, -0.0019],\n",
      "        [ 0.0015, -0.0031,  0.0005, -0.0019],\n",
      "        [ 0.0014, -0.0027,  0.0008, -0.0012],\n",
      "        [ 0.0015, -0.0031,  0.0005, -0.0017],\n",
      "        [ 0.0013, -0.0032,  0.0008, -0.0013],\n",
      "        [ 0.0014, -0.0031,  0.0004, -0.0016],\n",
      "        [ 0.0009, -0.0023, -0.0002, -0.0015]])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69279b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9, 9])\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.6249, 0.6251, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.4167, 0.4168, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.3125, 0.3126, 0.3125, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.2499, 0.2500, 0.2501, 0.2499, 0.2501, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.2082, 0.2083, 0.2084, 0.2082, 0.2084, 0.2084, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.1785, 0.1785, 0.1786, 0.1785, 0.0000, 0.1786, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.1562, 0.1562, 0.1563, 0.1562, 0.0000, 0.1563, 0.1562, 0.1563,\n",
      "          0.0000],\n",
      "         [0.0000, 0.1388, 0.1390, 0.0000, 0.1389, 0.1389, 0.1389, 0.1389,\n",
      "          0.1390]],\n",
      "\n",
      "        [[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.6239, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.4155, 0.4170, 0.4176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.3117, 0.3129, 0.3136, 0.3118, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.2493, 0.0000, 0.2506, 0.0000, 0.2505, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.2078, 0.2085, 0.2088, 0.0000, 0.2088, 0.2084, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.1781, 0.0000, 0.1790, 0.1781, 0.0000, 0.1787, 0.1783, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.1564, 0.0000, 0.1558, 0.1566, 0.1563, 0.1560, 0.1565,\n",
      "          0.0000],\n",
      "         [0.0000, 0.1390, 0.1392, 0.1385, 0.0000, 0.1389, 0.1386, 0.1391,\n",
      "          0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "print(attention_weights.shape)\n",
    "print(attention_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
