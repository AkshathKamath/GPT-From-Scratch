{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38638d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090b4b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "Number of tokens: 9\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "## Simple tokenization by splitting on spaces, ideally more complex tokenization would be used like BPE or WordPiece\n",
    "sentence = sentence.split()\n",
    "n = len(sentence)\n",
    "\n",
    "print(f\"Tokenized sentence: {sentence}\")\n",
    "print(f\"Number of tokens: {len(sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc915b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings (4-dimensional):\n",
      "  The : [1.  0.5 0.2 0.8]\n",
      "  quick: [0.3 1.  0.7 0.1]\n",
      "  brown: [0.6 0.2 1.  0.4]\n",
      "  fox : [0.9 0.8 0.3 1. ]\n",
      "  jumps: [0.4 0.6 0.8 0.2]\n",
      "  over: [0.7 0.3 0.5 0.9]\n",
      "  the : [1.  0.5 0.2 0.8]\n",
      "  lazy: [0.2 0.9 0.4 0.6]\n",
      "  dog : [0.8 0.4 0.9 0.3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Sample word embeddings, ideally these would be learned in the language modelling process or loaded from a pre-trained model like GloVe or Word2Vec\n",
    "\n",
    "embeddings = np.array([\n",
    "        [1.0, 0.5, 0.2, 0.8], \n",
    "        [0.3, 1.0, 0.7, 0.1],  \n",
    "        [0.6, 0.2, 1.0, 0.4],  \n",
    "        [0.9, 0.8, 0.3, 1.0],  \n",
    "        [0.4, 0.6, 0.8, 0.2],  \n",
    "        [0.7, 0.3, 0.5, 0.9],  \n",
    "        [1.0, 0.5, 0.2, 0.8],  \n",
    "        [0.2, 0.9, 0.4, 0.6],  \n",
    "        [0.8, 0.4, 0.9, 0.3]  \n",
    "    ])\n",
    "\n",
    "print(\"Word embeddings (4-dimensional):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  {word:4}: {embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7d7e2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embeddings (Same dimesnions as word embeddings):\n",
      "  Pos 0 (The): [0. 1. 0. 1.]\n",
      "  Pos 1 (quick): [0.1 0.9 0.1 0.9]\n",
      "  Pos 2 (brown): [0.2 0.8 0.2 0.8]\n",
      "  Pos 3 (fox): [0.3 0.7 0.3 0.7]\n",
      "  Pos 4 (jumps): [0.4 0.6 0.4 0.6]\n",
      "  Pos 5 (over): [0.5 0.5 0.5 0.5]\n",
      "  Pos 6 (the): [0.6 0.4 0.6 0.4]\n",
      "  Pos 7 (lazy): [0.7 0.3 0.7 0.3]\n",
      "  Pos 8 (dog): [0.8 0.2 0.8 0.2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Sample positional encodings, typically these would be generated using math functions or learned during training or RoPE\n",
    "\n",
    "positional_embeddings = np.array([\n",
    "    [0.0, 1.0, 0.0, 1.0],  \n",
    "    [0.1, 0.9, 0.1, 0.9],  \n",
    "    [0.2, 0.8, 0.2, 0.8],  \n",
    "    [0.3, 0.7, 0.3, 0.7],  \n",
    "    [0.4, 0.6, 0.4, 0.6],  \n",
    "    [0.5, 0.5, 0.5, 0.5],  \n",
    "    [0.6, 0.4, 0.6, 0.4],  \n",
    "    [0.7, 0.3, 0.7, 0.3],  \n",
    "    [0.8, 0.2, 0.8, 0.2]   \n",
    "])\n",
    "\n",
    "print(\"Positional embeddings (Same dimesnions as word embeddings):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  Pos {i} ({word}): {positional_embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d742a8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings (word + positional):\n",
      "  The : [1.  1.5 0.2 1.8]\n",
      "  quick: [0.4 1.9 0.8 1. ]\n",
      "  brown: [0.8 1.  1.2 1.2]\n",
      "  fox : [1.2 1.5 0.6 1.7]\n",
      "  jumps: [0.8 1.2 1.2 0.8]\n",
      "  over: [1.2 0.8 1.  1.4]\n",
      "  the : [1.6 0.9 0.8 1.2]\n",
      "  lazy: [0.9 1.2 1.1 0.9]\n",
      "  dog : [1.6 0.6 1.7 0.5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## The final input to the Attention block is the sum of the word embeddings and positional encodings\n",
    "\n",
    "input_embeddings = embeddings + positional_embeddings\n",
    "\n",
    "print(\"Input embeddings (word + positional):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  {word:4}: {input_embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fbc9026",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 4      # embedding dimension of the tokens\n",
    "num_heads = 2    # number of attention heads\n",
    "d_k = d_model // num_heads # Dimension of the Q, K and V matrices for each head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cfef11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight matrices:\n",
      "W_q (Query weights) shape: (2, 4, 2)\n",
      "[[[ 0.14901425 -0.04147929]\n",
      "  [ 0.19430656  0.45690896]\n",
      "  [-0.07024601 -0.07024109]\n",
      "  [ 0.47376384  0.23023042]]\n",
      "\n",
      " [[-0.14084232  0.16276801]\n",
      "  [-0.13902531 -0.13971893]\n",
      "  [ 0.07258868 -0.57398407]\n",
      "  [-0.51747535 -0.16868626]]]\n",
      "\n",
      "W_k (Key weights) shape: (2, 4, 2)\n",
      "[[[-0.30384934  0.0942742 ]\n",
      "  [-0.27240722 -0.42369111]\n",
      "  [ 0.43969463 -0.06773289]\n",
      "  [ 0.02025846 -0.42742446]]\n",
      "\n",
      " [[-0.16331482  0.03327678]\n",
      "  [-0.34529807  0.11270941]\n",
      "  [-0.18019161 -0.08750812]\n",
      "  [-0.18051198  0.55568346]]]\n",
      "\n",
      "W_v (Value weights) shape: (2, 4, 2)\n",
      "[[[-0.00404917 -0.31731328]\n",
      "  [ 0.24676347 -0.36625309]\n",
      "  [ 0.06265908 -0.58790104]\n",
      "  [-0.39845581  0.05905837]]\n",
      "\n",
      " [[ 0.22153997  0.05141048]\n",
      "  [-0.03469448 -0.09033111]\n",
      "  [-0.4435566  -0.21595326]\n",
      "  [-0.13819163  0.31713667]]]\n",
      "\n",
      "W_o (Output projection weights) shape: (4, 4)\n",
      "[[ 0.10308549 -0.52891205  0.09722519 -0.11552468]\n",
      " [-0.2030766   0.18350289  0.30929986  0.27938404]\n",
      " [-0.25176526 -0.09276371  0.09937903  0.29266354]\n",
      " [-0.14375227 -0.05569769 -0.33190049 -0.35886199]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "## Shape of the weights is (num_heads, d_model, d_k) for Q, K, V and (d_model, d_model) for output projection\n",
    "W_q = np.random.randn(num_heads, d_model, d_k) * 0.3  # Query wieghts\n",
    "W_k = np.random.randn(num_heads, d_model, d_k) * 0.3  # Key weights\n",
    "W_v = np.random.randn(num_heads, d_model, d_k) * 0.3  # Value weights\n",
    "W_o = np.random.randn(d_model, d_model) * 0.3 # Output projection weights. This is applied after concatenating the heads so that the head outputs can interact information with each other.\n",
    "\n",
    "print(\"Weight matrices:\")\n",
    "print(f\"W_q (Query weights) shape: {W_q.shape}\")\n",
    "print(W_q)\n",
    "print(f\"\\nW_k (Key weights) shape: {W_k.shape}\")\n",
    "print(W_k)\n",
    "print(f\"\\nW_v (Value weights) shape: {W_v.shape}\")\n",
    "print(W_v)\n",
    "print()\n",
    "\n",
    "print(f\"W_o (Output projection weights) shape: {W_o.shape}\")\n",
    "print(W_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "093477fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # Subtract max for numerical stability\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    # Normalize over rows to get probabilities i.e. sum of each row = 1 and for each token, we have a pdf over all tokens\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94ac313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To concatenate the outputs of each of the self attention heads, we will store them in this list. This will be of shape (num_heads, n, d_k)\n",
    "head_outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c0aea3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HEAD 1 ===\n",
      "Head 1 - Computing Q, K, V using weight matrices:\n",
      "  Q_h = input_embeddings @ W_q[0]  ->  shape: (9, 2)\n",
      "  K_h = input_embeddings @ W_k[0]  ->  shape: (9, 2)\n",
      "  V_h = input_embeddings @ W_v[0]  ->  shape: (9, 2)\n",
      "\n",
      "=== HEAD 2 ===\n",
      "Head 2 - Computing Q, K, V using weight matrices:\n",
      "  Q_h = input_embeddings @ W_q[1]  ->  shape: (9, 2)\n",
      "  K_h = input_embeddings @ W_k[1]  ->  shape: (9, 2)\n",
      "  V_h = input_embeddings @ W_v[1]  ->  shape: (9, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for head in range(num_heads):\n",
    "    print(f\"=== HEAD {head + 1} ===\")\n",
    "    \n",
    "    # Compute Q, K, V for this specific head only\n",
    "    Q_h = input_embeddings @ W_q[head]  # (n, d_k)\n",
    "    K_h = input_embeddings @ W_k[head]  # (n, d_k)\n",
    "    V_h = input_embeddings @ W_v[head]  # (n, d_k)\n",
    "    \n",
    "    print(f\"Head {head + 1} - Computing Q, K, V using weight matrices:\")\n",
    "    print(f\"  Q_h = input_embeddings @ W_q[{head}]  ->  shape: {Q_h.shape}\")\n",
    "    print(f\"  K_h = input_embeddings @ W_k[{head}]  ->  shape: {K_h.shape}\")\n",
    "    print(f\"  V_h = input_embeddings @ W_v[{head}]  ->  shape: {V_h.shape}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65e16e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HEAD 1 ===\n",
      "Head 1 - Attention scores (Q_h @ K_h.T) scaled by sqrt(d_k): (9, 9)\n",
      "Head 1 - Attention weights after softmax: (9, 9)\n",
      "Head 1 - Output (attention_weights_h @ V_h): (9, 2)\n",
      "\n",
      "=== HEAD 2 ===\n",
      "Head 2 - Attention scores (Q_h @ K_h.T) scaled by sqrt(d_k): (9, 9)\n",
      "Head 2 - Attention weights after softmax: (9, 9)\n",
      "Head 2 - Output (attention_weights_h @ V_h): (9, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for head in range(num_heads):\n",
    "    print(f\"=== HEAD {head + 1} ===\")\n",
    "\n",
    "    Q_h = input_embeddings @ W_q[head]\n",
    "    K_h = input_embeddings @ W_k[head]  \n",
    "    V_h = input_embeddings @ W_v[head]\n",
    "\n",
    "    attention_scores_h = Q_h @ K_h.T # (n, n)\n",
    "    attention_scores_h /= np.sqrt(d_k) # Scale the scores\n",
    "    print(f\"Head {head + 1} - Attention scores (Q_h @ K_h.T) scaled by sqrt(d_k): {attention_scores_h.shape}\")\n",
    "\n",
    "    attention_weights_h = softmax(attention_scores_h) # (n, n)\n",
    "    print(f\"Head {head + 1} - Attention weights after softmax: {attention_weights_h.shape}\")\n",
    "\n",
    "    output_h = attention_weights_h @ V_h # (n, d_k)\n",
    "    print(f\"Head {head + 1} - Output (attention_weights_h @ V_h): {output_h.shape}\")\n",
    "\n",
    "    head_outputs.append(output_h)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcb14b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked head outputs shape: (2, 9, 2)\n"
     ]
    }
   ],
   "source": [
    "head_outputs = np.array(head_outputs) # (num_heads, n, d_k)\n",
    "print(f\"Stacked head outputs shape: {head_outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "182efb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated output from all heads: (9, 4)\n"
     ]
    }
   ],
   "source": [
    "## Concatenate the output of each head along the last dimension i.e. over the d_k dimension (rows). num_heads * d_k = d_model\n",
    "\n",
    "concatenated_output = np.concatenate(head_outputs, axis=1) # (n, d_model)\n",
    "print(f\"Concatenated output from all heads: {concatenated_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4a3ce0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output after output projection (concatenated_output @ W_o): (9, 4)\n"
     ]
    }
   ],
   "source": [
    "final_output = concatenated_output @ W_o # (n, d_model)\n",
    "print(f\"Final output after output projection (concatenated_output @ W_o): {final_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "235f1508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.34494302 -0.16594008 -0.49273973 -0.51357179]\n",
      " [ 0.34647487 -0.15909348 -0.4852738  -0.50635228]\n",
      " [ 0.34336687 -0.15293441 -0.48028235 -0.50077979]\n",
      " [ 0.34598795 -0.16394967 -0.48996047 -0.51109125]\n",
      " [ 0.34236345 -0.15115023 -0.47936929 -0.49953431]\n",
      " [ 0.34188169 -0.15456373 -0.48264006 -0.50284664]\n",
      " [ 0.33998404 -0.15521293 -0.4845477  -0.50426322]\n",
      " [ 0.34235727 -0.15255953 -0.480753   -0.50093792]\n",
      " [ 0.33840776 -0.14368761 -0.47391596 -0.49318787]]\n"
     ]
    }
   ],
   "source": [
    "print(final_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
