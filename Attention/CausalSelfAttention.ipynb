{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "201cbc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your journey starts with one step\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Your journey starts with one step\"\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2474b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4829f3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360df1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1600, 0.1694, 0.1684, 0.1712, 0.1491, 0.1820],\n",
      "        [0.1614, 0.1673, 0.1661, 0.1730, 0.1477, 0.1845],\n",
      "        [0.1619, 0.1671, 0.1660, 0.1727, 0.1488, 0.1835],\n",
      "        [0.1631, 0.1674, 0.1666, 0.1707, 0.1541, 0.1781],\n",
      "        [0.1720, 0.1625, 0.1630, 0.1661, 0.1750, 0.1613],\n",
      "        [0.1580, 0.1695, 0.1680, 0.1732, 0.1429, 0.1884]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sa = SelfAttention(d_in=3, d_out=2)\n",
    "queries = sa.W_query(inputs) #A\n",
    "keys = sa.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad4e60ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In language modelling, we want to mask out the future tokens when training the model to predict the next token.\n",
    "## This is done by setting the attention weights to zero for future tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d2fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length)) ## Lower triangular mask where all values above the diagonal are zero\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0312a7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1600, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1614, 0.1673, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1619, 0.1671, 0.1660, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1631, 0.1674, 0.1666, 0.1707, 0.0000, 0.0000],\n",
      "        [0.1720, 0.1625, 0.1630, 0.1661, 0.1750, 0.0000],\n",
      "        [0.1580, 0.1695, 0.1680, 0.1732, 0.1429, 0.1884]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_masked = attn_weights*mask_simple ## Element-wise multiplication to apply the mask\n",
    "print(attn_weights_masked) ## The pdf is disturbed and we need to normalize the attention weights again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f75a7d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4910, 0.5090, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3271, 0.3376, 0.3354, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2442, 0.2506, 0.2495, 0.2556, 0.0000, 0.0000],\n",
      "        [0.2051, 0.1938, 0.1943, 0.1981, 0.2086, 0.0000],\n",
      "        [0.1580, 0.1695, 0.1680, 0.1732, 0.1429, 0.1884]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Normalize the attention weights after masking\n",
    "row_sums = attn_weights_masked.sum(dim=1, keepdim=True)\n",
    "attn_weights_masked = attn_weights_masked / row_sums\n",
    "print(attn_weights_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b759c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## While this method works, the information of the attention scores from future tokens is leaking into the attention weights of the current and previous tokens when we do the softmax operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f5585ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1) ## Upper triangular mask where all values below the diagonal are zero\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d0d968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.5897e-02,        -inf,        -inf,        -inf,        -inf,\n",
      "                -inf],\n",
      "        [-7.2614e-02, -2.1653e-02,        -inf,        -inf,        -inf,\n",
      "                -inf],\n",
      "        [-7.1101e-02, -2.6383e-02, -3.5499e-02,        -inf,        -inf,\n",
      "                -inf],\n",
      "        [-4.4235e-02, -7.3606e-03, -1.3702e-02,  2.0537e-02,        -inf,\n",
      "                -inf],\n",
      "        [-2.3816e-02, -1.0412e-01, -1.0012e-01, -7.3068e-02,  1.8775e-04,\n",
      "                -inf],\n",
      "        [-6.1314e-02,  3.7585e-02,  2.5256e-02,  6.8604e-02, -2.0406e-01,\n",
      "          1.8710e-01]], grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "attn_scores_masked = attn_scores.masked_fill(mask.bool(), -torch.inf) ## Mask out the future tokens by setting their attention scores to -inf, so that when we apply softmax, they will have zero attention weight. This prevents data leakage from future tokens.\n",
    "print(attn_scores_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdb94d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4910, 0.5090, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3271, 0.3376, 0.3354, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2442, 0.2506, 0.2495, 0.2556, 0.0000, 0.0000],\n",
      "        [0.2051, 0.1938, 0.1943, 0.1981, 0.2086, 0.0000],\n",
      "        [0.1580, 0.1695, 0.1680, 0.1732, 0.1429, 0.1884]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores_masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a11f6922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      " \n",
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "## Dropout\n",
    "example = torch.ones(6, 6)\n",
    "print(example)\n",
    "print(\" \")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) ## 50% of the elements will be set to zero and remaining will be scaled by 1/(1-0.5) = 2\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "863c62c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6541, 0.6751, 0.6708, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5013, 0.4990, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3876, 0.0000, 0.3962, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3390, 0.3360, 0.3465, 0.2857, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Dropout is applied after the attention weights are computed, but before they are used to compute the context vector.\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dbca9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0) ## We process the inputs in batches, so we stack the inputs along a new dimension\n",
    "print(batch.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) ## The upper triangular mask is registered as a buffer so that it is not treated as a parameter, but is still moved to the correct device (CPU/GPU) when the model is moved.\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        keys = self.W_key(x) # Shape b x num_tokens x d_out. Multiplication with broadcasting\n",
    "        queries = self.W_query(x) # Shape b x num_tokens x d_out\n",
    "        values = self.W_value(x) # Shape b x num_tokens x d_out\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Transpose the keys to get the shape from b x num_tokens x d_out to shape b x d_out x num_tokens and then multiply with queries to get the shape b x num_tokens x num_tokens\n",
    "        attn_scores.masked_fill_( \n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size ()\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1 # Shape b x num_tokens x num_tokens\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) ## Applying dropout to the attention weights\n",
    "\n",
    "        context_vec = attn_weights @ values # Shape b x num_tokens x d_out\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d83a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape) # Shape b x num_tokens x d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ca06dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs: tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"context_vecs:\", context_vecs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
