{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63e87ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0efad6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "Number of tokens: 9\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "## Simple tokenization by splitting on spaces, ideally more complex tokenization would be used like BPE or WordPiece\n",
    "sentence = sentence.split()\n",
    "n = len(sentence)\n",
    "\n",
    "print(f\"Tokenized sentence: {sentence}\")\n",
    "print(f\"Number of tokens: {len(sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d3f21f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings (4-dimensional):\n",
      "  The : tensor([1.0000, 0.5000, 0.2000, 0.8000])\n",
      "  quick: tensor([0.3000, 1.0000, 0.7000, 0.1000])\n",
      "  brown: tensor([0.6000, 0.2000, 1.0000, 0.4000])\n",
      "  fox : tensor([0.9000, 0.8000, 0.3000, 1.0000])\n",
      "  jumps: tensor([0.4000, 0.6000, 0.8000, 0.2000])\n",
      "  over: tensor([0.7000, 0.3000, 0.5000, 0.9000])\n",
      "  the : tensor([1.0000, 0.5000, 0.2000, 0.8000])\n",
      "  lazy: tensor([0.2000, 0.9000, 0.4000, 0.6000])\n",
      "  dog : tensor([0.8000, 0.4000, 0.9000, 0.3000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Sample word embeddings, ideally these would be learned in the language modelling process or loaded from a pre-trained model like GloVe or Word2Vec\n",
    "\n",
    "# Shape of embeddings: (n, d) where n is number of tokens and d is embedding dimension\n",
    "embeddings = torch.tensor([\n",
    "        [1.0, 0.5, 0.2, 0.8], \n",
    "        [0.3, 1.0, 0.7, 0.1],  \n",
    "        [0.6, 0.2, 1.0, 0.4],  \n",
    "        [0.9, 0.8, 0.3, 1.0],  \n",
    "        [0.4, 0.6, 0.8, 0.2],  \n",
    "        [0.7, 0.3, 0.5, 0.9],  \n",
    "        [1.0, 0.5, 0.2, 0.8],  \n",
    "        [0.2, 0.9, 0.4, 0.6],  \n",
    "        [0.8, 0.4, 0.9, 0.3]  \n",
    "    ])\n",
    "\n",
    "print(\"Word embeddings (4-dimensional):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  {word:4}: {embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6754b8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embeddings (Same dimesnions as word embeddings):\n",
      "  Pos 0 (The): tensor([0., 1., 0., 1.])\n",
      "  Pos 1 (quick): tensor([0.1000, 0.9000, 0.1000, 0.9000])\n",
      "  Pos 2 (brown): tensor([0.2000, 0.8000, 0.2000, 0.8000])\n",
      "  Pos 3 (fox): tensor([0.3000, 0.7000, 0.3000, 0.7000])\n",
      "  Pos 4 (jumps): tensor([0.4000, 0.6000, 0.4000, 0.6000])\n",
      "  Pos 5 (over): tensor([0.5000, 0.5000, 0.5000, 0.5000])\n",
      "  Pos 6 (the): tensor([0.6000, 0.4000, 0.6000, 0.4000])\n",
      "  Pos 7 (lazy): tensor([0.7000, 0.3000, 0.7000, 0.3000])\n",
      "  Pos 8 (dog): tensor([0.8000, 0.2000, 0.8000, 0.2000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Sample positional encodings, typically these would be generated using math functions or learned during training or RoPE\n",
    "\n",
    "positional_embeddings = torch.tensor([\n",
    "    [0.0, 1.0, 0.0, 1.0],  \n",
    "    [0.1, 0.9, 0.1, 0.9],  \n",
    "    [0.2, 0.8, 0.2, 0.8],  \n",
    "    [0.3, 0.7, 0.3, 0.7],  \n",
    "    [0.4, 0.6, 0.4, 0.6],  \n",
    "    [0.5, 0.5, 0.5, 0.5],  \n",
    "    [0.6, 0.4, 0.6, 0.4],  \n",
    "    [0.7, 0.3, 0.7, 0.3],  \n",
    "    [0.8, 0.2, 0.8, 0.2]   \n",
    "])\n",
    "\n",
    "print(\"Positional embeddings (Same dimesnions as word embeddings):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  Pos {i} ({word}): {positional_embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a5b23a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings (word + positional):\n",
      "  The : tensor([1.0000, 1.5000, 0.2000, 1.8000])\n",
      "  quick: tensor([0.4000, 1.9000, 0.8000, 1.0000])\n",
      "  brown: tensor([0.8000, 1.0000, 1.2000, 1.2000])\n",
      "  fox : tensor([1.2000, 1.5000, 0.6000, 1.7000])\n",
      "  jumps: tensor([0.8000, 1.2000, 1.2000, 0.8000])\n",
      "  over: tensor([1.2000, 0.8000, 1.0000, 1.4000])\n",
      "  the : tensor([1.6000, 0.9000, 0.8000, 1.2000])\n",
      "  lazy: tensor([0.9000, 1.2000, 1.1000, 0.9000])\n",
      "  dog : tensor([1.6000, 0.6000, 1.7000, 0.5000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## The final input to the Attention block is the sum of the word embeddings and positional encodings\n",
    "\n",
    "input_embeddings = embeddings + positional_embeddings\n",
    "\n",
    "print(\"Input embeddings (word + positional):\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"  {word:4}: {input_embeddings[i]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3fb849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo tensor before dropout:tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "Demo tensor after dropout:tensor([[0., 2., 0., 0., 2., 2.],\n",
      "        [0., 0., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 2., 0., 2., 2., 2.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.]])\n"
     ]
    }
   ],
   "source": [
    "## Demo of dropout that will be added intermittently in the model\n",
    "demo = torch.ones(6, 6)\n",
    "print(f\"Demo tensor before dropout:{demo}\")\n",
    "\n",
    "dropout = nn.Dropout(p=0.5) ## Randomly zeroes 50% of the elements in the input tensor during training and doubles the remaining elements to maintain the expected value\n",
    "demo_dropped = dropout(demo)\n",
    "print(f\"Demo tensor after dropout:{demo_dropped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d61d456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings shape after dropout: torch.Size([9, 4])\n",
      "Input embeddings after dropout:tensor([[1.2500, 1.8750, 0.2500, 0.0000],\n",
      "        [0.5000, 0.0000, 1.0000, 1.2500],\n",
      "        [0.0000, 1.2500, 1.5000, 1.5000],\n",
      "        [0.0000, 1.8750, 0.7500, 2.1250],\n",
      "        [1.0000, 1.5000, 0.0000, 1.0000],\n",
      "        [1.5000, 1.0000, 0.0000, 0.0000],\n",
      "        [2.0000, 1.1250, 1.0000, 1.5000],\n",
      "        [0.0000, 0.0000, 1.3750, 0.0000],\n",
      "        [2.0000, 0.0000, 0.0000, 0.6250]])\n"
     ]
    }
   ],
   "source": [
    "dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "input_embeddings = dropout(input_embeddings)\n",
    "print(f\"Input embeddings shape after dropout: {input_embeddings.shape}\")\n",
    "print(f\"Input embeddings after dropout:{input_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f888914",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = input_embeddings.shape[1]  # Embedding dimension\n",
    "d_k = 3 # Dimension of keys and queries (generally kept smaller to make Q, K and V matrices low rank for efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2787aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_q (Query weights) shape: torch.Size([4, 3])\n",
      "tensor([[ 0.1010,  0.0386,  0.0703],\n",
      "        [ 0.0691, -0.3369, -0.0559],\n",
      "        [ 0.6625, -0.1914,  0.1385],\n",
      "        [ 0.0802,  0.1605,  0.2428]])\n",
      "\n",
      "W_k (Key weights) shape: torch.Size([4, 3])\n",
      "tensor([[ 0.3331, -0.5069, -0.2967],\n",
      "        [ 0.2874,  0.3966,  0.2452],\n",
      "        [-0.2298, -0.2252,  0.4058],\n",
      "        [ 0.2059, -0.0983,  0.2385]])\n",
      "\n",
      "W_v (Value weights) shape: torch.Size([4, 3])\n",
      "tensor([[ 0.0845,  0.0168,  0.1568],\n",
      "        [-0.0715, -0.0150,  0.1579],\n",
      "        [-0.0025,  0.2187,  0.0399],\n",
      "        [ 0.2592, -0.3047, -0.2666]])\n",
      "\n",
      "W_o (Output projection weights) shape: torch.Size([3, 4])\n",
      "tensor([[ 0.0449, -0.0627, -0.1161,  0.2974],\n",
      "        [ 0.1404, -0.0615, -0.2223,  0.1086],\n",
      "        [ 0.5760, -0.0676, -0.1025,  0.0912]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)  # For reproducible results\n",
    "\n",
    "## Shape of the Q, K and V matrices is d x d_k and for the output projection matrix is d_k x d to project the attention output back to d dimensions\n",
    "W_q = torch.randn(d_model, d_k, dtype=torch.float32) * 0.3  \n",
    "W_k = torch.randn(d_model, d_k, dtype=torch.float32) * 0.3  \n",
    "W_v = torch.randn(d_model, d_k, dtype=torch.float32) * 0.3  \n",
    "W_o = torch.randn(d_k, d_model, dtype=torch.float32) * 0.3 \n",
    "\n",
    "print(f\"W_q (Query weights) shape: {W_q.shape}\")\n",
    "print(W_q)\n",
    "print(f\"\\nW_k (Key weights) shape: {W_k.shape}\")\n",
    "print(W_k)\n",
    "print(f\"\\nW_v (Value weights) shape: {W_v.shape}\")\n",
    "print(W_v)\n",
    "print()\n",
    "\n",
    "print(f\"W_o (Output projection weights) shape: {W_o.shape}\")\n",
    "print(W_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db4dd9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q (Queries) shape: torch.Size([9, 3])\n",
      "tensor([[ 0.4214, -0.6312,  0.0177],\n",
      "        [ 0.8132,  0.0285,  0.4772],\n",
      "        [ 1.2004, -0.4675,  0.5021],\n",
      "        [ 0.7968, -0.4342,  0.5150],\n",
      "        [ 0.2849, -0.3062,  0.2293],\n",
      "        [ 0.2206, -0.2789,  0.0496],\n",
      "        [ 1.0625, -0.2524,  0.5805],\n",
      "        [ 0.9109, -0.2632,  0.1904],\n",
      "        [ 0.2521,  0.1776,  0.2924]])\n",
      "\n",
      "K (Keys) shape: torch.Size([9, 3])\n",
      "tensor([[ 0.8978,  0.0537,  0.1903],\n",
      "        [ 0.1942, -0.6016,  0.5555],\n",
      "        [ 0.3235,  0.0105,  1.2728],\n",
      "        [ 0.8041,  0.3659,  1.2708],\n",
      "        [ 0.9701, -0.0103,  0.3095],\n",
      "        [ 0.7870, -0.3638, -0.1999],\n",
      "        [ 1.0686, -0.9403,  0.4459],\n",
      "        [-0.3159, -0.3096,  0.5579],\n",
      "        [ 0.7949, -1.0753, -0.4443]])\n",
      "\n",
      "V (Values) shape: torch.Size([9, 3])\n",
      "tensor([[-0.0291,  0.0477,  0.5021],\n",
      "        [ 0.3637, -0.1537, -0.2149],\n",
      "        [ 0.2956, -0.1477, -0.1426],\n",
      "        [ 0.4148, -0.5115, -0.2406],\n",
      "        [ 0.2364, -0.3103,  0.1270],\n",
      "        [ 0.0552,  0.0103,  0.3931],\n",
      "        [ 0.4747, -0.2215,  0.1313],\n",
      "        [-0.0035,  0.3007,  0.0549],\n",
      "        [ 0.3309, -0.1567,  0.1470]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q = torch.matmul(input_embeddings, W_q) # Shape: (n, d_k)  \n",
    "K = torch.matmul(input_embeddings, W_k) # Shape: (n, d_k)\n",
    "V = torch.matmul(input_embeddings, W_v) # Shape: (n, d_k)\n",
    "\n",
    "print(f\"\\nQ (Queries) shape: {Q.shape}\")\n",
    "print(Q)\n",
    "print(f\"\\nK (Keys) shape: {K.shape}\")\n",
    "print(K)\n",
    "print(f\"\\nV (Values) shape: {V.shape}\")\n",
    "print(V)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a460513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask shape: torch.Size([9, 9])\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "## Creating the mask for causal attention where each token can only attend to itself and previous tokens, where there is a 1 in the mask matrix, the attention score is kept, where there is a 0 the attention score is masked out\n",
    "\n",
    "mask = torch.tril(torch.ones(n, n))  # Shape: (n, n)\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7378667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores shape: torch.Size([9, 9])\n",
      "tensor([[ 3.4782e-01,  4.7136e-01,  1.5225e-01,  1.3049e-01,  4.2082e-01,\n",
      "          5.5773e-01,  1.0517e+00,  7.2186e-02,  1.0058e+00],\n",
      "        [ 8.2241e-01,  4.0583e-01,  8.7070e-01,  1.2707e+00,  9.3629e-01,\n",
      "          5.3428e-01,  1.0550e+00,  4.9726e-04,  4.0372e-01],\n",
      "        [ 1.1481e+00,  7.9320e-01,  1.0224e+00,  1.4322e+00,  1.3247e+00,\n",
      "          1.0144e+00,  1.9462e+00,  4.5658e-02,  1.2337e+00],\n",
      "        [ 7.9005e-01,  7.0201e-01,  9.0872e-01,  1.1364e+00,  9.3689e-01,\n",
      "          6.8212e-01,  1.4894e+00,  1.7005e-01,  8.7140e-01],\n",
      "        [ 2.8292e-01,  3.6687e-01,  3.8077e-01,  4.0842e-01,  3.5047e-01,\n",
      "          2.8974e-01,  6.9455e-01,  1.3274e-01,  4.5378e-01],\n",
      "        [ 1.9251e-01,  2.3817e-01,  1.3157e-01,  1.3839e-01,  2.3224e-01,\n",
      "          2.6516e-01,  5.2012e-01,  4.4340e-02,  4.5321e-01],\n",
      "        [ 1.0508e+00,  6.8061e-01,  1.0799e+00,  1.4997e+00,  1.2130e+00,\n",
      "          8.1200e-01,  1.6316e+00,  6.6359e-02,  8.5801e-01],\n",
      "        [ 8.3986e-01,  4.4097e-01,  5.3425e-01,  8.7813e-01,  9.4528e-01,\n",
      "          7.7456e-01,  1.3057e+00, -1.0002e-01,  9.2241e-01],\n",
      "        [ 2.9155e-01,  1.0459e-01,  4.5564e-01,  6.3933e-01,  3.3328e-01,\n",
      "          7.5394e-02,  2.3285e-01,  2.8517e-02, -1.2047e-01]])\n"
     ]
    }
   ],
   "source": [
    "attention_scores = torch.matmul(Q, K.T)  # Shape: (n, n)\n",
    "\n",
    "print(f\"Attention scores shape: {attention_scores.shape}\")\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61b42b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Masked attention scores shape: torch.Size([9, 9])\n",
      "tensor([[ 0.3478,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf],\n",
      "        [ 0.8224,  0.4058,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf],\n",
      "        [ 1.1481,  0.7932,  1.0224,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf],\n",
      "        [ 0.7900,  0.7020,  0.9087,  1.1364,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf],\n",
      "        [ 0.2829,  0.3669,  0.3808,  0.4084,  0.3505,    -inf,    -inf,    -inf,\n",
      "            -inf],\n",
      "        [ 0.1925,  0.2382,  0.1316,  0.1384,  0.2322,  0.2652,    -inf,    -inf,\n",
      "            -inf],\n",
      "        [ 1.0508,  0.6806,  1.0799,  1.4997,  1.2130,  0.8120,  1.6316,    -inf,\n",
      "            -inf],\n",
      "        [ 0.8399,  0.4410,  0.5342,  0.8781,  0.9453,  0.7746,  1.3057, -0.1000,\n",
      "            -inf],\n",
      "        [ 0.2915,  0.1046,  0.4556,  0.6393,  0.3333,  0.0754,  0.2329,  0.0285,\n",
      "         -0.1205]])\n"
     ]
    }
   ],
   "source": [
    "masked_attention_scores = attention_scores.clone()\n",
    "\n",
    "## Wherever the mask is 0, set the attention score to -inf so that after softmax, the attention weights becomes 0 for the future tokens\n",
    "masked_attention_scores = masked_attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "print(f\"\\nMasked attention scores shape: {masked_attention_scores.shape}\")\n",
    "print(masked_attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7c8c8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled masked attention scores shape: torch.Size([9, 9])\n",
      "tensor([[ 0.2008,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf],\n",
      "        [ 0.4748,  0.2343,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf],\n",
      "        [ 0.6628,  0.4580,  0.5903,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf],\n",
      "        [ 0.4561,  0.4053,  0.5246,  0.6561,    -inf,    -inf,    -inf,    -inf,\n",
      "            -inf],\n",
      "        [ 0.1633,  0.2118,  0.2198,  0.2358,  0.2023,    -inf,    -inf,    -inf,\n",
      "            -inf],\n",
      "        [ 0.1111,  0.1375,  0.0760,  0.0799,  0.1341,  0.1531,    -inf,    -inf,\n",
      "            -inf],\n",
      "        [ 0.6067,  0.3929,  0.6235,  0.8659,  0.7003,  0.4688,  0.9420,    -inf,\n",
      "            -inf],\n",
      "        [ 0.4849,  0.2546,  0.3084,  0.5070,  0.5458,  0.4472,  0.7539, -0.0577,\n",
      "            -inf],\n",
      "        [ 0.1683,  0.0604,  0.2631,  0.3691,  0.1924,  0.0435,  0.1344,  0.0165,\n",
      "         -0.0696]])\n"
     ]
    }
   ],
   "source": [
    "masked_attention_scores /= torch.sqrt(torch.tensor(d_k, dtype=torch.float32)) # Scale by sqrt(d_k)\n",
    "\n",
    "print(f\"\\nScaled masked attention scores shape: {masked_attention_scores.shape}\")\n",
    "print(masked_attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e06b78c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention weights shape: torch.Size([9, 9])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5598, 0.4402, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3643, 0.2968, 0.3388, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2357, 0.2240, 0.2524, 0.2879, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1915, 0.2010, 0.2026, 0.2059, 0.1991, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1659, 0.1703, 0.1602, 0.1608, 0.1698, 0.1730, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1335, 0.1078, 0.1358, 0.1731, 0.1467, 0.1163, 0.1867, 0.0000, 0.0000],\n",
      "        [0.1321, 0.1049, 0.1107, 0.1350, 0.1404, 0.1272, 0.1729, 0.0768, 0.0000],\n",
      "        [0.1144, 0.1027, 0.1258, 0.1399, 0.1172, 0.1010, 0.1106, 0.0983, 0.0902]])\n"
     ]
    }
   ],
   "source": [
    "attention_weights = F.softmax(masked_attention_scores, dim=-1)\n",
    "\n",
    "print(f\"\\nAttention weights shape: {attention_weights.shape}\")\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fda5e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention weights after dropout shape: torch.Size([9, 9])\n",
      "tensor([[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6998, 0.5502, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4554, 0.0000, 0.4235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2946, 0.2800, 0.3155, 0.3598, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2533, 0.2573, 0.2489, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2074, 0.0000, 0.2002, 0.2010, 0.2122, 0.2163, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1348, 0.1698, 0.2163, 0.1833, 0.1454, 0.2334, 0.0000, 0.0000],\n",
      "        [0.1651, 0.1312, 0.1384, 0.1688, 0.1755, 0.0000, 0.2161, 0.0000, 0.0000],\n",
      "        [0.1430, 0.1284, 0.1572, 0.1748, 0.0000, 0.1262, 0.1382, 0.1229, 0.1127]])\n"
     ]
    }
   ],
   "source": [
    "## Adding dropout to the attention weights\n",
    "dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "attention_weights = dropout(attention_weights)\n",
    "\n",
    "print(f\"\\nAttention weights after dropout shape: {attention_weights.shape}\")\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1c0ad15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention output shape: torch.Size([9, 3])\n",
      "tensor([[-0.0364,  0.0596,  0.6276],\n",
      "        [ 0.1797, -0.0512,  0.2331],\n",
      "        [ 0.1119, -0.0408,  0.1682],\n",
      "        [ 0.3358, -0.2597, -0.0438],\n",
      "        [ 0.2404, -0.2463, -0.0664],\n",
      "        [ 0.1986, -0.1861,  0.1392],\n",
      "        [ 0.3511, -0.2635,  0.0059],\n",
      "        [ 0.2979, -0.2214,  0.0450],\n",
      "        [ 0.2710, -0.1356,  0.0708]])\n"
     ]
    }
   ],
   "source": [
    "output = torch.matmul(attention_weights, V)  # Shape: (n, d_k)\n",
    "\n",
    "print(f\"\\nAttention output shape: {output.shape}\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4aa88f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final output shape: torch.Size([9, 4])\n",
      "tensor([[ 0.3682, -0.0438, -0.0733,  0.0529],\n",
      "        [ 0.1351, -0.0239, -0.0334,  0.0691],\n",
      "        [ 0.0962, -0.0159, -0.0212,  0.0442],\n",
      "        [-0.0466, -0.0021,  0.0232,  0.0677],\n",
      "        [-0.0620,  0.0046,  0.0336,  0.0387],\n",
      "        [ 0.0630, -0.0104,  0.0040,  0.0516],\n",
      "        [-0.0178, -0.0062,  0.0172,  0.0763],\n",
      "        [ 0.0082, -0.0081,  0.0100,  0.0687],\n",
      "        [ 0.0339, -0.0134, -0.0086,  0.0723]])\n"
     ]
    }
   ],
   "source": [
    "final_output = torch.matmul(output, W_o) # Shape: (n, d)\n",
    "\n",
    "## Projecting the output back to d dimensions\n",
    "print(f\"\\nFinal output shape: {final_output.shape}\")\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ceb48341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final output after dropout shape: torch.Size([9, 4])\n",
      "tensor([[ 0.4602, -0.0000, -0.0000,  0.0661],\n",
      "        [ 0.1689, -0.0298, -0.0000,  0.0864],\n",
      "        [ 0.1202, -0.0198, -0.0265,  0.0552],\n",
      "        [-0.0583, -0.0026,  0.0290,  0.0846],\n",
      "        [-0.0775,  0.0057,  0.0420,  0.0484],\n",
      "        [ 0.0787, -0.0130,  0.0051,  0.0644],\n",
      "        [-0.0223, -0.0077,  0.0215,  0.0954],\n",
      "        [ 0.0103, -0.0101,  0.0125,  0.0858],\n",
      "        [ 0.0000, -0.0168, -0.0107,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "## Adding dropout to the final attention output\n",
    "dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "final_output = dropout(final_output)\n",
    "\n",
    "print(f\"\\nFinal output after dropout shape: {final_output.shape}\")\n",
    "print(final_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
