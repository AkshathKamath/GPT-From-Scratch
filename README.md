Implementing core NLP and LLM topics from scratch in Pytorch.

- Attention
  - Self Attention
  - Multi Head Self Attention
  - Masked Self Attention
- Transformer
  - Complete Transformer block with residual connections, layer norm and feed forward networks
  - Final language model head and softmax to generate pdf for each token over vocabulary
